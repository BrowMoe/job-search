\section{Intermediate results}

\section{Detailed proofs}

The $L^{2}$ risk can be written :
\[\mathds{E}_{\theta^{\circ}}^{n}\left[\left\Vert \overline{\theta}^{\widehat{m}} - \theta^{\circ} \right\Vert^{2}\right] = \mathds{E}_{\theta^{\circ}}^{n}\left[\sum\limits_{j = 1}^{G_{n}} \left( \overline{\theta}^{\widehat{m}}_{j} - \theta^{\circ}_{j} \right)^{2}\right] + \mathds{E}_{\theta^{\circ}}^{n}\left[\sum\limits_{j = G_{n} + 1}^{\infty} \left(\theta^{\circ}_{j}\right)^{2}\right].\]
Together with
\[\forall j \in \llbracket 1, G_{n} \rrbracket, \quad \overline{\theta}^{\widehat{m}}_{j} - \theta^{\circ}_{j} = \left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j}\right) \mathds{1}_{\{\widehat{m} \in \llbracket j, G_{n} \rrbracket\}} + \theta^{\circ}_{j} \mathds{1}_{\{\widehat{m} \in \llbracket 1, j-1 \rrbracket\}},\]
implies that

\begin{alignat*}{3}
& \mathds{E}_{\theta^{\circ}}^{n}\left[\left\Vert \overline{\theta}^{\widehat{m}} - \theta^{\circ} \right\Vert^{2}\right] &&\leq&& \underbrace{\sum\limits_{j = 1}^{G_{n}}\mathds{E}_{\theta^{\circ}}^{n}\left[\left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j}\right)^{2} \mathds{1}_{\{\widehat{m} \in \llbracket j, G_{n} \rrbracket\}}\right]}_{=: A}\\
& && && + \underbrace{\sum\limits_{j = 1}^{G_{n}}\mathds{E}_{\theta^{\circ}}^{n}\left[\left(\theta^{\circ}_{j}\right)^{2} \mathds{1}_{\{\widehat{m} \in \llbracket 1, j-1 \rrbracket\}}\right]}_{=: B}+ \underbrace{\sum\limits_{j = G_{n} + 1}^{\infty}\mathds{E}_{\theta^{\circ}}^{n}\left[\left( \theta^{\circ}_{j}\right)^{2}\right]}_{=: C}.
\end{alignat*}

\bigskip

We will now control each of the three parts of the sum using \textsc{\cref{lmA.1.2}} and \textsc{\cref{prB.1.1}}.

\bigskip

First, consider $A$ and let be some positive real number $p$ to be specified later.

Then we can write
\begin{alignat*}{3}
& A && \leq && \sum\limits_{j = 1}^{G_{n}^{+}} \mathds{E}_{\theta^{\circ}}^{n}\left[\left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j}\right)^{2}\right] + \sum\limits_{j = G_{n}^{+} + 1}^{G_{n}} \mathds{E}_{\theta^{\circ}}^{n} \left[ \left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j} \right)^{2} \mathds{1}_{\{\widehat{m} \in \llbracket j, G_{n} \rrbracket\}}\right]\\
& &&\leq&& \sum\limits_{j = 1}^{G_{n}^{+}} \mathds{E}_{\theta^{\circ}}^{n}\left[\left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j}\right)^{2}\right] + \sum\limits_{j = G_{n}^{+} + 1}^{G_{n}} \mathds{E}_{\theta^{\circ}}^{n} \left[ \left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j} \right)^{2} \mathds{1}_{\{\widehat{m} \in \llbracket G_{n}^{+}+1, G_{n} \rrbracket\}}\right]\\
& &&\leq&& \sum\limits_{j = 1}^{G_{n}^{+}} \mathds{E}_{\theta^{\circ}}^{n}\left[\left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j}\right)^{2}\right] + \sum\limits_{j = G_{n}^{+} + 1}^{G_{n}} \mathds{E}_{\theta^{\circ}}^{n} \left[ \left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j} \right)^{2} \mathds{1}_{\{\widehat{m} \in \llbracket G_{n}^{+}+1, G_{n} \rrbracket\}}\right] \\
& && && - p\mathds{E}_{\theta^{\circ}}^{n}\left[\mathds{1}_{\{\widehat{m} \in \llbracket G_{n}^{+}+1, G_{n} \rrbracket}\right] + p \mathds{E}_{\theta^{\circ}}^{n}\left[\mathds{1}_{\{\widehat{m} \in \llbracket G_{n}^{+}+1, G_{n} \rrbracket}\right]\\
& &&\leq&& \sum\limits_{j = 1}^{G_{n}^{+}} \mathds{E}_{\theta^{\circ}}^{n}\left[\left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j}\right)^{2}\right] + \mathds{E}_{\theta^{\circ}}^{n} \left[ \left( \sum\limits_{j = G_{n}^{+} + 1}^{G_{n}}\left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j} \right)^{2} - p\right) \mathds{1}_{\{\widehat{m} \in \llbracket G_{n}^{+}+1, G_{n} \rrbracket\}}\right]\\
& && &&+ p \mathds{E}_{\theta^{\circ}}^{n}\left[\mathds{1}_{\{\widehat{m} \in \llbracket G_{n}^{+}+1, G_{n} \rrbracket}\right]\\
& &&\leq&& \underbrace{\sum\limits_{j = 1}^{G_{n}^{+}} \mathds{E}_{\theta^{\circ}}^{n}\left[\left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j}\right)^{2}\right]}_{=: A_{3}} + \underbrace{\mathds{E}_{\theta^{\circ}}^{n}\left[ \left(\sum\limits_{j = G_{n}^{+} + 1}^{G_{n}}\left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j} \right)^{2} - p\right)_{+}\right]}_{=: A_{1}} + \underbrace{p \mathds{E}_{\theta^{\circ}}^{n}\left[\mathds{1}_{\{\widehat{m} \in \llbracket G_{n}^{+}+1, G_{n} \rrbracket}\right]}_{=: A_{2}} .
\end{alignat*}

\medskip

First, we will control $A_{1}$ using \textsc{\cref{lmA.1.2}}.

The goal is to give $p$ a value that is large enough to control this object but small enough so $p \cdot \mathds{P}_{\theta^{\circ}}^{n} \left[G_{n}^{+} < \widehat{m} \leq G_{n}\right]$ is still for the most $\Phi_{n}^{\circ}$.

Define $S_{n} := \sum\limits_{j = G_{n}^{+} + 1}^{G_{n}}\left(\frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j}\right)^{2}.$

We have, for all $j$ in $\llbracket G_{n}^{+} + 1, G_{n} \rrbracket$,
\[ \frac{Y_{j}}{\lambda_{j}} - \theta^{\circ}_{j} \sim \mathcal{N}\left(0, \frac{\Lambda_{j}}{n} \right), \]
so $\mathds{E}_{\theta^{\circ}}^{n}\left[S_{n}\right] = \frac{1}{n} \sum\limits_{j = G_{n}^{+} + 1}^{G_{n}} \Lambda_{j}$.

And define, using the definition of $G_{n}$ and $\textsc{\cref{as2.4.2}}$
\begin{alignat*}{3}
& t_{m} &&:=&& \Lambda_{1} \geq \frac{\Lambda_{G_{n}}}{n} \geq \max\limits_{j \in \llbracket G_{n}^{+} + 1, G_{n} \rrbracket} \frac{\Lambda_{j}}{n}\\
& v_{m} && := && G_{n} \Lambda_{1} \geq \frac{G_{n} \Lambda_{G_{n}}}{n} \geq  \frac{1}{n} \sum\limits_{j = G_{n}^{+} + 1}^{G_{n}} \Lambda_{j}.
\end{alignat*}

We can take $p = \mathds{E}_{\theta^{\circ}}^{n}\left[S_{n}\right] + 3 v_{m}$ which gives, using the definition of $G_{n}^{+}$ and $G_{n} > G_{n}^{+}$
\begin{alignat*}{3}
& A_{1} && = && \mathds{E}_{\theta^{\circ}}^{n}\left[\left(S_{n} - \mathds{E}_{\theta^{\circ}}^{n}\left[S_{n}\right] - 3 v_{m}\right)_{+}\right]\\
& &&\leq&& 6 \Lambda_{1} \exp\left[- \frac{G_{n}}{2}\right]\\
& &&\leq&& 6 \Lambda_{1} \exp\left[- \frac{n G_{n}}{2n}\right]\\
& &&\leq&& 6 \Lambda_{1} \exp\left[- n \frac{3 \Lambda_{m_{n}^{\circ}}^{-1} \Phi_{n}^{\circ}}{2} - \frac{m_{n}^{\circ}}{2}\right]\\
& A_{1} && \leq && 6 \Lambda_{1} \exp\left[- \frac{2 m_{n}^{\circ}}{L}\right].
\end{alignat*}

\medskip

Thanks to \textsc{\cref{prB.1.1}} it is easily shown that
\[A_{2} < 4 \Lambda_{1} \exp\left[-\frac{5 m_{n}^{\circ}}{9 L} + 2 \log \left(G_{n}\right)\right].\]

\medskip

Finally, we control $A_{3}.$
Using the definition of $G_{n}^{+}$ we have

\begin{alignat*}{3}
& A_{3} &&=&& \sum\limits_{j = 1}^{G_{n}^{+}} \mathds{E}_{\theta^{\circ}}^{n}\left[\left(\overline{\theta}_{j} - \theta^{\circ}_{j}\right)^{2}\right]\\
& &&=&& \sum\limits_{j = 1}^{G_{n}^{+}} \frac{\Lambda_{j}}{n}\\
& &&=&& \frac{1}{n} G_{n}^{+} \overline{\Lambda}_{G_{n}^{+}}\\
&A_{3} &&\leq&& \frac{\overline{\Lambda}_{G_{n}^{+}}}{\Lambda_{m_{n}^{\circ}}} 3 \Phi_{n}^{\circ}.
\end{alignat*}

Note that, using \textsc{\cref{as2.4.3}} and the definition of $G_{n}^{+}$, we have that $\frac{\overline{\Lambda}_{G_{n}^{+}}}{\Lambda_{m_{n}^{\circ}}}$ is bounded for $n$ large enough; indeed with $D^{\circ} := \left\lceil \frac{3}{\kappa^{\circ}} + 1\right\rceil$,

\begin{alignat*}{3}
& G_{n}^{+} && \leq && \frac{3 n \Phi_{n}^{\circ}}{\Lambda_{m_{n}^{\circ}}} + m_{n}^{\circ} \leq \frac{3 n m_{n}^{\circ} \overline{\Lambda}_{m_{n}^{\circ}}}{\Lambda_{m_{n}^{\circ}} n \kappa^{\circ}} + m_{n}^{\circ} \leq \left(\frac{3}{\kappa^{\circ}} + 1\right) m_{n}^{\circ} \leq D^{\circ} m_{n}^{\circ}\\ 
& &&\Rightarrow&& \frac{\overline{\Lambda}_{G_{n}^{+}}}{\Lambda_{m_{n}^{\circ}}}\leq \frac{\overline{\Lambda}_{D^{\circ} \cdot m_{n}^{\circ}}}{\Lambda_{D^{\circ} \cdot m_{n}^{\circ}}} \cdot \frac{\Lambda_{D^{\circ} \cdot m_{n}^{\circ}}}{\Lambda_{m_{n}^{\circ}}} \leq \Lambda_{D^{\circ}}.
\end{alignat*}

\medskip

Hence, we have
\[A \leq 6 \Lambda_{1} \exp\left[- \frac{2 m_{n}^{\circ}}{L}\right] + 4 \Lambda_{1} \exp\left[-\frac{5 m_{n}^{\circ}}{9 L} + 2 \log \left(G_{n}\right)\right] +  \Lambda_{D^{\circ}} 3 \Phi_{n}^{\circ}.\]

\bigskip

Now we control $B$ We use a decomposition similar to the one used for $A$ :
\begin{alignat*}{3}
& B && \leq && \sum\limits_{j = G_{n}^{-} + 1}^{G_{n}} \left(\theta^{\circ}_{j}\right)^{2} + \sum\limits_{j = 1}^{G_{n}^{-}} \mathds{E}_{\theta^{\circ}}^{n}\left[\left(\theta^{\circ}_{j}\right)^{2} \mathds{1}_{\left\{\widehat{m} \in \llbracket 1, j - 1 \rrbracket\right\}}\right]\\
& && \leq && \sum\limits_{j = G_{n}^{-} + 1}^{G_{n}} \left(\theta^{\circ}_{j} \right)^{2} + \sum\limits_{j = 1}^{G_{n}^{-}} \mathds{E}_{\theta^{\circ}}^{n}\left[\left(\theta^{\circ}_{j}\right)^{2} \mathds{1}_{\left\{\widehat{m} \in \llbracket 1, G_{n}^{-} - 1 \rrbracket\right\}}\right]\\
& B && \leq && \underbrace{\sum\limits_{j = G_{n}^{-} + 1}^{G_{n}} \left(\theta^{\circ}_{j}\right)^{2}}_{=: B_{1}} + \underbrace{\sum\limits_{j = 1}^{G_{n}^{-}} \left(\theta^{\circ}_{j}\right)^{2} \mathds{P}_{\theta^{\circ}}^{n}\left[1 \leq \widehat{m} < G_{n}^{-}\right]}_{=: B_{2}}.
\end{alignat*}

\medskip

First, notice that $B_{1} + C = \mathfrak{b}_{G_{n}^{-}} \leq 9 L \Phi_{n}^{\circ}$ by the definition of $G_{n}^{-}.$

\medskip

To control $B_{2},$ we use the fact that $\theta^{\circ}$ is square summable and the \textsc{\cref{prB.1.1}} :
 
\begin{alignat*}{3}
& B_{2} &&=&& \mathds{P}_{\theta^{\circ}}^{n}\left[1 \leq \widehat{m} < G_{n}^{-}\right] \sum\limits_{j = 1}^{G_{n}^{-}} \left(\theta^{\circ}_{j}\right)^{2} \\
& &&\leq&& \exp\left[-\frac{7 m_{n}^{\circ}}{9} + \log\left(G_{n}\right)\right] \cdot \Vert \theta^{\circ} \Vert^{2}.
%& &&\leq&& \exp\left[-\frac{7 m_{\epsilon}^{\circ}}{9} + \log\left(G_{\epsilon}\right)\right] \cdot \mathfrak{a}_{1}\sum\limits_{j = 1}^{G_{\epsilon}^{-}} \frac{1}{\mathfrak{a}_{j}}\left(\theta^{\circ}_{j} - \theta^{\times}_{j}\right)^{2}\\
%& &&\leq&& \exp\left[-\frac{7 m_{\epsilon}^{\circ}}{9} + \log\left(G_{\epsilon}\right)\right] \cdot \mathfrak{a}_{1}L^{\circ}\\
 \end{alignat*}

\medskip

So we have
\[B + C \leq 9 L \Phi_{n}^{\circ} + \Vert \theta^{\circ} \Vert^{2} \cdot \exp\left[-\frac{7 m_{n}^{\circ}}{9} + \log\left(G_{n}\right)\right].\]

\bigskip

Which leads to :
\begin{alignat*}{3}
&\mathds{E}_{\theta^{\circ}}^{n}\left[\left\Vert \overline{\theta}^{\widehat{m}} - \theta^{\circ} \right\Vert^{2}\right] &&\leq&& 3 \left( \Lambda_{D^{\circ}} + 3 L \right) \Phi_{n}^{\circ} +\\
& && &&\left(6 \Lambda_{1} \exp\left[- \frac{2 m_{n}^{\circ}}{L} - \log\left(\Phi_{n}^{\circ}\right)\right] + 4 \Lambda_{1} \exp\left[-\frac{5 m_{n}^{\circ}}{9 L} + \log \left(\frac{G_{n}^{2}}{\Phi_{n}^{\circ}}\right)\right] \right.\\
& && && \left. + \Vert \theta^{\circ} \Vert^{2} \cdot \exp\left[-\frac{7 m_{n}^{\circ}}{9} + \log\left(\frac{G_{n}}{\Phi_{n}^{\circ}}\right)\right]\right) \Phi_{n}^{\circ},\\
\end{alignat*}

which proves that there exist $C$ such that, for $n$ large enough,
\[\mathds{E}_{\theta^{\circ}}^{n}\left[\left\Vert \overline{\theta}^{\widehat{m}} - \theta^{\circ} \right\Vert^{2}\right] \leq C \Phi_{n}^{\circ}.\]

%\bigskip
%To conclude, in the case where there are more than one minima to the penalised contrast (note $\widehat{m}_{min}$ and $\widehat{m}_{max}$ the smallest and the greatest minimising values and $\overline{\theta}^{\widehat{m}_{min}}$ and $\overline{\theta}^{\widehat{m}_{max}}$ the associated projection estimators), we can use the following inequality :
%
%\begin{alignat*}{3}
%& \Vert \overline{\theta} - \theta^{\circ} \Vert^{2} && = && \Vert \overline{\theta}^{\widehat{m}_{min}} - \theta^{\circ, \widehat{m}_{min}} \Vert^{2} + \mathfrak{b}_{\widehat{m}_{max}}^{2} + \sum\limits_{j = \widehat{m}_{min} + 1}^{\widehat{m}_{max}} \left(\overline{\theta}_{j} - \theta^{\circ}_{j}\right)^{2}\\
%& &&\leq&& \Vert \overline{\theta}^{\widehat{m}_{min}} - \theta^{\circ, \widehat{m}_{min}} \Vert^{2} + \mathfrak{b}_{\widehat{m}_{max}}^{2} + \sum\limits_{j = \widehat{m}_{min} + 1}^{\widehat{m}_{max}} \max\left(\left(\overline{\theta}_{j}^{\widehat{m}_{max}} - \theta^{\circ}_{j}\right)^{2}, \left(\overline{\theta}_{j}^{\widehat{m}_{min}} - \theta^{\circ}_{j}\right)^{2}\right)\\
%& &&\leq&& \Vert \overline{\theta}^{\widehat{m}_{min}} - \theta^{\circ, \widehat{m}_{min}} \Vert_{l_{2}}^{2} + \mathfrak{b}_{\widehat{m}_{max}}^{2} + \sum\limits_{j = \widehat{m}_{min} + 1}^{\widehat{m}_{max}} \left(\left(\overline{\theta}_{j}^{\widehat{m}_{max}} - \theta^{\circ}_{j}\right)^{2} + \left(\overline{\theta}_{j}^{\widehat{m}_{min}} - \theta^{\circ}_{j}\right)^{2}\right)\\
%& &&\leq&& \Vert \overline{\theta}^{\widehat{m}_{min}} - \theta^{\circ} \Vert^{2} + \Vert \overline{\theta}^{\widehat{m}_{max}} - \theta^{\circ} \Vert^{2}.
%\end{alignat*}
%
%And we conclude with
%
%\begin{alignat*}{3}
%& \mathds{E}_{\theta^{\circ}}\left[\Vert \overline{\theta} - \theta^{\circ}\Vert^{2}\right] && \leq && \mathds{E}_{\theta^{\circ}}\left[\Vert \overline{\theta}^{\widehat{m}_{min}} - \theta^{\circ} \Vert^{2} + \Vert \overline{\theta}^{\widehat{m}_{max}} - \theta^{\circ} \Vert^{2}\right] \\
%& &&\leq&& \mathds{E}_{\theta^{\circ}}\left[\Vert \overline{\theta}^{\widehat{m}_{min}} - \theta^{\circ} \Vert^{2} \right]+ \mathds{E}_{\theta^{\circ}}^{n}\left[\Vert \overline{\theta}^{\widehat{m}_{max}} - \theta^{\circ} \Vert^{2}\right]\\
%& &&\leq&& 2 C \Phi_{n}^{\circ}
%\end{alignat*}
%and we set $C^{\circ} := 2C.$
%
%Note that this inequality has the interest to proof that any convex combination of some minimax optimal estimators is also a minimax optimal estimator.