\section{Frequentist approach}\label{INTRO_FREQ}
In the two previous sections, we have first introduced inverse problems in a general context and highlighted some difficulties which are inherent to this kind of problem.
We then introduced the type of data we will have at hand.
Now, we aim to introduce the methods we will use and more generally the paradigm they conform to, what motivates their construction and how to justify satisfaction or dissatisfaction regarding their properties.
As explained in the introduction, our methods will be of two kinds, namely frequentist and Bayesian.
In this section, we present the frequentist paradigm and the notions of decision theory which allow to quantify the quality of frequentist estimation methods.

\subsection{Estimation}\label{INTRO_FREQ_ESTIMATION}

Remind that, given a family of probability distributions on $\Xi$ and indexed by $\Xi$ itself $(\P_{x})_{x \in \Xi}$ we are interested in estimating an object $f$ in $\Xi$ while observing some data $Y$ from $\P_{g}$ where $g = Tf$ with $T$ a linear operator from $\Xi$ onto itself.
Then, the frequentist approach consists in defining an estimator of the parameter of interest using the data where an estimator is an application as defined hereafter.

\begin{de}
Given a parameter space $(\Xi, \mathcal{A})$ and an observation space $(\mathds{Y}, \mathcal{Y})$, an estimator is a measurable application from $(\mathds{Y}, \mathcal{Y})$ to $(\Xi, \mathcal{A})$.
\assEnd
\end{de}

Hence, in our particular case, an estimator would be any measurable application from $(\Xi^{n}, \mathcal{A}^{\otimes n})$ to $(\Xi, \mathcal{A})$.
As mentioned earlier, using the generalised Fourier transform, we will go through the space of sequences $\Theta$, equipped with the Borel sigma algebra generated by the $l^{2}$-norm, say $\mathcal{B}$.
In our context, some naive estimators for relevant objects of the model we consider are the so-called "empirical estimators" or "orthogonal series estimator" (OSE).

\begin{de}
Keeping in mind that we observe $Y^{n} = (Y_{p})_{p \in \llbracket 1, n \rrbracket}$ where $(Y_{p})_{p \in \mathds{Z}}$ is a stationary process such that, for any $p$ in $\mathds{Z}$, we have $Y_{p}$ follows $\P_{g}$, where $(\P_{x})_{x \in \Xi}$ is a probability distribution on $\Xi$.
Define, for any $s$ in $\mathds{F}$
\begin{alignat*}{10}
&\phi_{n}(s) && : && (\Xi^{n}, \mathcal{A}^{\otimes n}) && \rightarrow && (\Theta, \mathcal{B}); && \quad \theta_{n}(s) && : && (\Xi^{n}, \mathcal{A}^{\otimes n}) && \rightarrow && (\Theta, \mathcal{B});\\
& && && Y^{n} && \mapsto && n^{-1} \sum\nolimits_{p = 1}^{n} \langle Y_{p} \vert e_{s} \rangle_{\Xi} && \quad && && Y^{n} && \mapsto && \phi_{n}(s) \lambda^{-1}(s)
\end{alignat*}
where $\lambda^{-1}$ is well defined as we assumed $\lambda(s) \neq 0$ for any $s$.
If it were not the case, one would use the generalised inverse $\lambda^{+}(s) = \lambda(s)^{-1} \mathds{1}_{\{\lambda(s) \neq 0\}}$.

Note that this definition is suitable under assumption \nref{AS_INTRO_DATA_KNOWN} but not \nref{AS_INTRO_DATA_UNKNOWN} as it relies on the knowledge of $\lambda$ to be computed; in this case we would consider
\begin{alignat*}{5}
& \theta_{n, n_{\lambda}}(s) && : && (\Xi^{n + n_{\lambda}}, \mathcal{A}^{\otimes (n + n_{\lambda})}) && \rightarrow && (\Theta, \mathcal{B});\\
& && && (Y^{n}, \epsilon^{n_{\lambda}}) && \mapsto && \phi_{n}(s) \lambda_{n_{\lambda}}^{+}(s)
\end{alignat*}
where we define, for any $s$ in $\mathds{F}$ the estimator $\lambda_{n_{\lambda}}(s) := n_{\lambda}^{-1} \sum_{p = 1}^{n_{\lambda}} \langle \epsilon_{p} \vert e_{s} \rangle_{\Xi}$ of $\lambda(s)$ and $\lambda_{n_{\lambda}}^{+}(s) := \mathds{1}_{\{\vert \lambda_{n_{\lambda}}(s) \vert^{2} > n_{\lambda}^{-1} \}} \lambda_{n_{\lambda}}^{-1}(s)$ which hence does not rely on the knowledge of $\lambda$ but the information we have about it through the observation of $\epsilon^{n_{\lambda}}$.

From these estimators one can naturally build their counterparts
\[g_{n}: Y^{n} \mapsto \mathcal{F}^{-1}(\phi_{n}) ; \quad h_{n_{\lambda}}: \epsilon^{n_{\lambda}} \mapsto \mathcal{F}^{-1}(\lambda_{n_{\lambda}}); \quad f_{n, n_{\lambda}}; (Y^{n}, \epsilon^{n_{\lambda}}) \mapsto \mathcal{F}^{-1}(\theta_{n, n_{\lambda}}).\]
\assEnd
\end{de}

However, we have seen in \nref{INTRO_INVERSEPROBLEMS} that inverse problems define a class of statistical models which has three major characteristics.
We have also seen that two of them (non-existence or non-unicity of the solution) can be addressed thanks to the generalised inverse construction.
However, we also pointed out that even once one has addressed those two issues, they can still face the difficulty of instability of the solution.

We will see that the estimators we just defined do not escape this phenomenon.

It is in order to address this issue that one defines the family of operators called regularisations.

\begin{de}\label{INTRO_FREQ_ESTIMATION_REGULARISATION}
Given $S$ in $\mathcal{L}(\Xi)$, a family of elements of $\mathcal{L}(\Xi)$, say $\{S_{m}^{+}, m \in \R_{+} \}$ is called regularisation of $S^{+}$ if, for any $x$ in $\mathcal{D}(S^{+})$ holds $\lim_{m \rightarrow \infty} \Vert S_{m}^{+} x - S^{+} x \Vert_{\Xi} = 0$.
\assEnd
\end{de}

Note that the definition of such a family does not solve the problem by itself.
Indeed, define the operator norm such that, for any $S$ in $\mathcal{L}(\Xi)$ we have, $\Vert S \Vert_{\mathcal{L}(\Xi)} := \sup\{\Vert S(x) \Vert_{\Xi}, x \in \Xi, \Vert x \Vert_{\Xi} \leq 1\}$.
Then, if $S^{+}$ is not bounded, then, for any regularisation of $S^{+}$, we have $\lim_{m \rightarrow \infty} \Vert S^{+}_{m} \Vert_{\mathcal{L}(\Xi)} = \infty$ and hence the limit itself is not an element of $\mathcal{L}(\Xi)$.

However, for any $S$ in $\mathcal{L}(\Xi)$, and $x$ in $\Xi$, if we have a sequence of estimates, indexed by an integer $n$, say, $(\widehat{S(x)}_{n})_{n \in \N}$ of $S(x)$ such that $\lim_{n \rightarrow \infty} \Vert \widehat{S(x)}_{n} - S(x) \Vert_{\Xi} = 0$, then, there exist a sequence $m_{n}$ such that $\lim_{n \rightarrow \infty} \Vert S^{+}_{m_{n}}(\widehat{S(x)}_{n}) - S^{+}(S(x)) \Vert_{\Xi} = 0$ and hence there exists a consistent estimation procedure.

Hence, we see that the selection of the parameter $m$, which we will call regularisation parameter, is primordial.
Depending on it, the estimation procedure could be consistent or not.
In addition, within the choices leading to consistent estimation, one can obtain various convergence rates.

In this thesis, the so called regularisation by dimension reduction plays a central role.

The regularisation consists in projecting our estimate onto the "lower frequencies" from $\mathcal{U}$.
To do so, consider the following definition.

\begin{de}
Consider an index set $\mathds{M}$ (here $\N$), and a sequence of measurable subsets of $\mathds{F}$ indexed by $\mathds{M}$, say, $(\mathds{F}_{m})_{m \in \mathds{M}}$.
This sequence is called a nested sieve if:
\item[\mylabel{BACKGROUND_REGULARISATION_NESTEDSIEVE_i}{\dgrau{\bfseries{i: }}}] for any $k$ and $m$  in $\mathds{M}$ such that $k \leq m$, we have $\mathds{F}_{k} \subset \mathds{F}_{m}$;
\item[\mylabel{BACKGROUND_REGULARISATION_NESTEDSIEVE_ii}{\dgrau{\bfseries{ii: }}}] for any $m$ in $\mathds{M}$, we have $\mu(\mathds{F}_{m}) < \infty$;
\item[\mylabel{BACKGROUND_REGULARISATION_NESTEDSIEVE_iii}{\dgrau{\bfseries{iii: }}}] $\cup_{m \in \mathds{M}} \mathds{F}_{m} = \mathds{F}$.
\assEnd
\end{de}

Similarly, for any $m$ in $\mathds{M}$, we define $\mathds{U}_{\overline{m}}$ the linear subspace of $\mathds{U}$ generated by $(e_{s})_{s \in \mathds{F}_{m}}$.

For any $m$ in $\mathds{M}$, we will denote  the set $\mathds{F} \setminus \mathds{F}_{m}$ by $\mathds{F}_{m}^{c}$.
In all the examples in this thesis, $\mathds{F}$ will be either $\N$ or $\mathds{Z}$; $\mathds{M}$ will be $\mathds{N}$; and for any $m$ in $\mathds{N}$, $\mathds{F}_{m}$ will be $\{s \in \mathds{F}: \vert s \vert \leq m \}$.
The following notation will hence be regularly used: for any $s_{1}$ and $s_{2}$ in $\mathds{Z}$ with $s_{1} \leq s_{2}$ we denote $\llbracket s_{1}, s_{2} \rrbracket$ the set $[s_{1}, s_{2}] \cap \mathds{Z}$.

By extension, for any $m_{1}$ and $m_{2}$ in $\mathds{M}$, we will denote $\mathds{U}_{\underline{m_{1}}}$ the linear subspace of $\mathds{U}$ generated by $(e_{s})_{s \in \mathds{F}_{m_{1}}^{c}}$; and $\mathds{U}_{\underline{m_{1}}, \overline{m_{2}}}$ the linear subspace of $\mathds{U}$ generated by $(e_{s})_{s \in \mathds{F}_{m_{1}}^{c} \cap \mathds{F}_{m_{2}}}$.
One should note that for any $m$ in $\mathds{M}$, $\mathds{U}_{\underline{m}}$ is the orthogonal complement of $\mathds{U}_{\overline{m}}$ in $\mathds{U}$.

Then, the following operators appear naturally.

\begin{de}
We define the following family of projection operators on $\Theta$.
For any $m_{1}$ and $m_{2}$ in $\mathds{M}$ denote by $\Pi_{\overline{m_{1}}}$, $\Pi_{\underline{m_{1}}}$, and $\Pi_{\underline{m_{1}}, \overline{m_{2}}}$ the following projection operators:
\begin{alignat*}{10}
& \Pi_{\overline{m_{1}}} && : && \Theta && \rightarrow && \Theta; \quad && \Pi_{\underline{m_{1}}} && : && \Theta && \rightarrow && \Theta;\\
& && && [x] && \mapsto && (s \mapsto [x](s) \mathds{1}_{\{ s \in \mathds{F}_{m_{1}}\}}) \quad && && && [x] && \mapsto && (s \mapsto [x](s) \mathds{1}_{\{ s \in \mathds{F}_{m_{1}}^{c}\}})\\
& \Pi_{\underline{m_{1}}, \overline{m_{2}}} && : && \Theta && \rightarrow && \Theta && && && && && \\
& && && [x] && \mapsto && (s \mapsto [x](s) \mathds{1}_{\{ s \in \mathds{F}_{m_{1}}^{c} \cap \mathds{F}_{m_{2}}^{c}\}}) && && && && &&
\end{alignat*}

By extension, we define, for any $m$ in $\mathds{M}$ the truncated Fourier transform $\mathcal{F}_{\overline{m}}$
\[ \mathcal{F}_{\overline{m}} : \Xi \rightarrow \Theta; \quad [x] \mapsto [x]_{\overline{m}} = (\Pi_{\overline{m}} [x] : s \mapsto [x](s) \mathds{1}_{\{s \in \mathds{F}_{m}\}}).\]

We see that $\mathcal{F}_{\overline{m}}$ is a unitary mapping between Hilbert spaces and we should highlight that its conjugates is given by
\[\mathcal{F}_{\overline{m}}^{\star} : \Theta \rightarrow \Xi, \quad [x] \mapsto \sum_{s \in \mathds{F}_{m}} [x](s) e_{s} = \Pi_{\mathds{U}_{\overline{m}}} \mathcal{F}^{\star}([x]) = \mathcal{F}^{\star}(\Pi_{\overline{m}}[x]).\]
 \assEnd
 \end{de}

Hence, considering an inverse problem where one is interested in estimating $f$ in $\Xi$ when having at hand an estimate $\widehat{T(f)}$ of $T(f)$ where $T$ is a bounded linear operator from $\Xi$ onto itself, we will consider the family of so called projection estimators defined by $\{\widehat{f}_{\overline{m}} = \Pi_{\mathds{U}_{\overline{m}}}(T^{+}\widehat{T(f)}), m \in \mathds{M}\}$.

We will see that, often, it will be easier to approximate objects in $\Theta$ and then apply $\mathcal{F}^{\star}$.
In this perspective we extend the definition of $\mathcal{F}$ in the following way.

\begin{de}
Denote $\mathcal{L}(\Theta)$ the space of linear application from $\Theta$ onto itself.
Then, for any $S$ in $\mathcal{L}(\Xi)$, we define $[S]$ to be
\begin{alignat*}{5}
& [S] && : && \mathds{F}^{2} && \rightarrow && \mathds{K}. \\
& && && (s_{1}, s_{2}) && \mapsto && [S](s_{1}, s_{2}) = \langle e_{s_{1}} \vert S(e_{s_{2}}) \rangle_{\Xi}
\end{alignat*}
Notice that $[S]$ defines an element of $\mathcal{L}(\Theta)$ such that, for any $[x]$ in $\Theta$, $[S][x]$ is such that, for any $s$ in $\mathds{F}$, $[S][x](s)$ is given by $\sum_{s' \in \mathds{F}} [S](s, s')[x](s')$.

In addition, we define, for any $m$ in $\mathds{M}$ and $[S]$ in $\mathcal{L}(\Theta)$, the operator $[S]_{\overline{m}}$ such that for any $s_{1}$ and $s_{2}$ in $\mathds{F}$, we have $[S]_{\overline{m}}(s_{1}, s_{2}) = [S](s_{1}, s_{2}) \mathds{1}_{\{ \{s_{1} \in \mathds{F}_{m}\} \cap \{s_{2}  \in \mathds{F}_{m}\} \}}$.
It is interesting to note that for any $S$ in $\mathcal{L}(\Xi)$ and $m$ in $\mathds{M}$, if we denote $S_{\overline{m}} = \Pi_{\mathds{U}_{\overline{m}}} S \Pi_{\mathds{U}_{\overline{m}}}$, we have $[S]_{\overline{m}} = [S_{\overline{m}}]$.

We note that the adjoint operator of $[S]$ is represented for any $s_{1}$ and $s_{2}$ in $\mathds{F}$ by $[S]^{\star}(s_{1}, s_{2}) = [S^{\star}](s_{1}, s_{2}) = \overline{[S](s_{2}, s_{1})}$.
\assEnd
\end{de}

Notice that, for the operator $T$ appearing in our model, due to \nref{INTRO_DATA_DIAGONAL}, we have for any $s$ and $s'$ in $\mathds{F}$ that $[T](s, s') = \mathds{1}_{\{s = s'\}} \lambda(s)$.
Considering the objects we just introduced, the following notations will be convenient throughout the thesis.

\begin{nota}\label{INTRO_FREQ_PROJEST}
For any $m$ in $\mathds{M}$ let be the following objects:
\begin{alignat*}{15}
& \lambda_{\overline{m}} && : && \mathds{F} && \rightarrow && \mathds{K}; && \quad \theta^{\circ}_{\overline{m}} && : && \mathds{F} && \rightarrow && \mathds{K}; && \quad \phi_{\overline{m}} && : && \mathds{F} && \rightarrow && \mathds{K}.\\
& && && s && \mapsto && \Pi_{\overline{m}} \lambda(s) && && && s && \mapsto && \Pi_{\overline{m}} \theta^{\circ}(s) && && && s && \mapsto && \Pi_{\overline{m}} \phi(s) = \lambda_{\overline{m}}(s) \theta^{\circ}(s)
\end{alignat*}
as well as their counterparts in $\Xi$
\[h_{\overline{m}} := \mathcal{F}^{-1}(\lambda_{\overline{m}}) \, ; \quad f_{\overline{m}} := \mathcal{F}^{-1}(\theta^{\circ}_{\overline{m}}); \quad g_{\overline{m}} := \mathcal{F}^{-1}(\phi^{\circ}_{\overline{m}}).\]

We also define their empirical counterparts which are called "projection estimators".
Under \nref{AS_INTRO_DATA_KNOWN} they take the following form:
\begin{alignat*}{10}
& \phi_{n, \overline{m}} && : && \mathds{F} && \rightarrow && \mathds{K}; && \quad \theta_{n, \overline{m}} && : && \mathds{F} && \rightarrow && \mathds{K};\\
& && && s && \mapsto && \Pi_{\overline{m}} \phi_{n}(s) && && && s && \mapsto && \Pi_{\overline{m}} \theta_{n}(s) = \lambda_{\overline{m}}^{-1}(s) \phi_{n}(s)
\end{alignat*}
and their counterparts in $\Xi$ are
\[g_{n, \overline{m}} := \mathcal{F}^{-1}(\phi_{n, \overline{m}}); \quad f_{n, \overline{m}} := \mathcal{F}^{-1}(\theta_{n, \overline{m}}).\]
On the other hand, under \nref{AS_INTRO_DATA_UNKNOWN} they take the form
\begin{alignat*}{15}
& \phi_{n, \overline{m}} && : && \mathds{F} && \rightarrow && \mathds{K}; && \quad \lambda_{n_{\lambda}} && : && \mathds{F} && \rightarrow && \mathds{K}; && \theta_{n, n_{\lambda}, \overline{m}} && : && \mathds{F} && \rightarrow && \mathds{K}.\\
& && && s && \mapsto && \Pi_{\overline{m}} \phi_{n}(s) && && && s && \mapsto && \lambda_{n_{\lambda}}(s) && && && s && \mapsto && \Pi_{\overline{m}} \theta_{n, n_{\lambda}}(s) = \lambda_{n_{\lambda}}^{+}(s) \phi_{n, \overline{m}}(s)
\end{alignat*}
where $\lambda_{n_{\lambda}}^{+}(s) = \mathds{1}_{\{ \vert\lambda_{n_{\lambda}}(s)\vert^{2} \geq n_{\lambda}^{-1}\}} \lambda_{n_{\lambda}}^{-1}(s)$, for any $s$ in $\mathds{F}$.
Their counterparts in $\Xi$ are
\[g_{n, \overline{m}} := \mathcal{F}^{-1}(\phi_{n, \overline{m}}); \quad f_{n, \overline{m}} := \mathcal{F}^{-1}(\theta_{n, \overline{m}}).\]
\assEnd
\end{nota}

The family $\{\lambda_{\overline{m}}, m \in \mathds{M}\}$ defines a regularisation as defined in \nref{INTRO_FREQ_ESTIMATION_REGULARISATION}.
We hence have at hand a family of estimators, called projection estimators, arising from the empirical estimators based on our data while using the dimension reduction regularisation technic.

Note that many other types of regularisations have gathered interest along the years.
For example \ncite{engl1989convergence} consider the convergence rate of Tikhonov regularisation; while \ncite{cavalier2007wavelet} consider the Galerkin regularisation.

The estimation technics we will study in this thesis are deeply linked to the family of projection estimators.
As one might notice, given a set of observations, the number of potential estimators for $f$ is infinite, and it can be easily seen that most of them do not lead to a consistent estimation.
Hence, we will be interested in properties which can objectively indicate if a given estimator is satisfying.

\subsection{Decision theory}\label{INTRO_FREQ_DECISION}

As we have seen previously, for a given model, one could chose among a variety of estimators.
This choice is in general not obvious and decision theory can be used to help in this process.

To make this part more illustrative for the remaining of this script let us first introduce the following set of assumptions about the parameter space that will hold true for all of our examples.

\begin{as}\label{INTRO_FREQ_DECISION_PARAMETERSPACE}
Assume that $\Xi$ is a subset of the space of functions from $[0, 1]$ to $\mathds{C}$, equipped with the scalar product $(x, y) \mapsto \langle x \vert y \rangle_{L^{2}} = \int_{[0, 1]} x(t) \cdot \overline{y(t)} \, \text{d}t$.
Then we consider $(e_{s})_{s \in \mathds{Z}} = ([0, 1] \rightarrow \mathds{C}, t \mapsto \exp[2 \cdot \imath \cdot \pi \cdot s \cdot t])_{s \in \mathds{Z}}$.
One can see that it is an orthonormal system in $\Xi$.
Hence, $\Theta$ is a subset of $\mathds{C}^{\mathds{Z}}$ equipped with the scalar product $([x], [y]) \mapsto \langle [x] \vert [y] \rangle_{l^{2}} = \sum_{s \in \mathds{Z}} [x](s) \cdot \overline{[y](s)}$.
\assEnd
\end{as}

\medskip

We have used, in the past sections, the distance between an estimate of an object of interest and the said object as an argument about whether one should be satisfied about the said estimate.
We formalise now the criteria under which one can qualify an estimator as satisfying.

\subsubsection{The loss function $l: (\{\mathds{Y} \rightarrow \Xi\} \times \mathds{Y} \times \Xi) \rightarrow \R_{+}$}\label{INTRO_FREQ_DECISION_LOSSFUNCION}
this function represents the error made by using a certain estimator $\widehat{f}$ while estimating the true parameter $f$ when the data at hand is $Y$.

A natural choice would be to consider a distance on $\Xi$, say $d: \Xi \times \Xi \rightarrow \R_{+}$ and to define $l : \{\mathds{Y} \rightarrow \Xi\} \times \mathds{Y} \times \Xi \rightarrow \R_{+}; \quad (\widehat{f}, Y, f) \mapsto d(\widehat{f}(Y), f).$

\medskip

Under \nref{INTRO_FREQ_DECISION_PARAMETERSPACE} it is natural to consider an element of the family of $L^{p}$ distances defined for any $p$ in $\R_{+}$ and $x$ and $y$ in $\Xi$ by $\Vert x - y \Vert_{L^{p}} = (\int_{[0, 1]} \vert x(t) - y(t) \vert^{p} \, \text{d}t)^{1/p}$ with the limit cases $\Vert x - y \Vert_{L^{\infty}} = \sup_{t \in [0, 1]} \{ \vert x(t) - y(t) \vert \}$ and $\Vert x - y\Vert_{L^{0}} = \int_{[0, 1]} \mathds{1}_{\{ \vert x(t) - y(t) \vert > 0\}} \, \text{d}t$.

In this thesis we will only consider the quadratic loss function $L^{2}$.
Notice, though, that our results could be easily generalised to the case where given a measurable function $\mathfrak{u}$ in $\Xi$, one considers for any $x$ in $\Xi$ its weighted norm $\Vert x \Vert_{L^{2}_{\mathfrak{u}}} = (\int_{[0, 1]} \vert (x \star \mathfrak{u})(t)\vert^{2} \, \text{d}t)^{1/2} = (\int_{[0, 1]} \vert (\int_{[0, 1]} x(v) \cdot \mathfrak{u}(t - v) \, \text{d}v)\vert^{2} \, \text{d}t)^{1/2}$ where $\star$ stands for the convolution operator on $\Xi$.
In addition, this type of norm will nonetheless play an important role later where we consider minimax optimality over Sobolev's ellipsoids.

\medskip

In order to apply decision theory, we have to assume that the object $f$ we try to estimate belongs to the space where the loss function is finite, for which we give the following notations.
\begin{de}\label{DE_INTRO_FREQ_SPACELXI}
Let $\mathds{L}^{2}$ be the subset of $\Xi$ such that $\mathds{L}^{2} := \{x \in \Xi: \Vert x \Vert_{L^{2}} < \infty\}$ and in addition, for any function $\mathfrak{u}$ in $\Xi$ and any $r$ in $\R_{+}$ let be $\mathds{L}_{\mathfrak{u}}^{2} := \{x \in \Xi: \Vert x \Vert_{L_{\mathfrak{u}}^{2}} < \infty\}$ and $\Xi_{\mathfrak{u}}(r) := \{ x \in \Xi: \Vert x \Vert_{L^{2}_{\mathfrak{u}}} < r\}$.
\assEnd
\end{de}


\bigskip

We have seen that we are interested in estimation methods which are based on the estimation of the Fourier transform of $f$, $\theta^{\circ}$.
In the case of the $L^{2}$-norm, we can see that considering the loss function on $\Theta$ is sufficient to quantify the performance on $\Xi$.
Indeed, let be the $l^{2}$-norm on $\Theta$ defined for any $[x]$ in $\Theta$ by $\Vert [x] \Vert_{l^{2}} = (\sum_{s \in \mathds{Z}} \vert [x](s) \vert^{2})^{1/2}$ and the associated space $\mathcal{L}^{2} = \{[x] \in \Xi: \Vert [x] \Vert_{l^{2}} < \infty\}$.
Given a sequence $[\mathfrak{u}]$ in $\Theta$, we can define the weighted norm which is given, for any $[x]$ in $\Theta$ by $\Vert [x] \Vert_{l^{2}_{[\mathfrak{u}]}} = (\sum_{s \in \mathds{Z}} \vert [x](s) [\mathfrak{u}](s) \vert^{2})^{1/2}$ and for any $r$ in $\R_{+}$ we define the associated space $\Theta({[\mathfrak{u}]}, r) := \{[x] \in \Theta: \Vert [x] \Vert_{l^{2}_{[\mathfrak{u}]}} < r\}$.

The theorem of Plancherel gives us the link between those distances, we have for any $x$ and $\mathfrak{u}$ in $\Xi$ and their Fourier transforms $[x]$ and $[\mathfrak{u}]$ the $\Vert x \Vert^{2}_{L^{2}_{\mathfrak{u}}} = \Vert [x] \Vert_{l_{[\mathfrak{u}]}^{2}}^{2}$.

We hence assume from now on that the parameter of interest has finite norm.

\begin{as}\label{AS_INTRO_FREQ_DECISION_THETAL2}
The parameter of interest $f$ is in $\mathds{L}^{2}$.
\assEnd
\end{as}

This assumption is equivalent to assuming that $\theta^{\circ}$ is in $\mathcal{L}^{2}$.

We shall highlight that this definition has to be adapted under \nref{AS_INTRO_DATA_UNKNOWN} where we obtain $ l : \{\mathds{Y}^{2} \rightarrow \Xi\} \times \mathds{Y}^{2} \times \Xi \rightarrow \R_{+}; \quad (\widehat{f}, Y, \epsilon, f) \mapsto d(\widehat{f}(Y, \epsilon), f)$.

\subsubsection{The risk function $\left(\mathcal{R}_{n} : (\{\mathds{Y} \rightarrow \Theta\} \times \Theta) \rightarrow \R_{+}\right)_{n \in \N}$}\label{INTRO_FREQ_DECISION_RISKFUNCION}
One can notice the the loss function defined previously depends on the observation and, as such, is a random object that cannot be optimised over the choice of estimator.

A way to overcome this limitation is considering a so called risk function such as the expected loss function $\mathcal{R}_{n}(\widehat{f}, f) = \E\left[l(\widehat{f}, Y^{n}, f)\right]$ or $\mathcal{R}_{n, n_{\lambda}}(\widehat{f}, f, h) = \E\left[l(\widehat{f}, Y^{n}, \epsilon^{n_{\lambda}}, f)\right]$ depending on the considered set of assumptions.

The following assumption, which will be verified in every model we consider allows us to obtain interesting upper bounds for the quadratic risk of projection estimators.
\begin{as}\label{as:il}
Assume that there exist constants $V_{1}$ and $V_{2}$ in $\R_{+}^{}\star$ such that, for any $s$ in $\mathds{F}$, we have $V_{1} \leq \V[\langle Y_{0} \vert e_{s} \rangle_{\Xi}] \leq V_{2}$.
In addition assume that there exist constants $V_{3}$, $V_{4}$, and $\cst{4}$ such that $V_{3} \leq \V[\langle \epsilon_{0} \vert e_{s} \rangle_{\Xi}] \leq V_{4}$ and $\ssE^2\FuEx\Vabs{\fedf[(s)]-\hfedf[(s)]}^4\leq\cst{4}$.
\assEnd
\end{as}

This hypothesis allows us to show the following result.

\begin{lm}\label{ge:oSv:re}
If \nref{as:il} holds true, then
\begin{inparaenum} 
\item[\mylabel{ge:oSv:re:ii}{{\dr\upshape(i)}}]
${\FuEx\Vabs{\fedf[(s)]\hfedfmpI[(s)]}^2}\leq 2 V_{4} + 1$;
\item[\mylabel{ge:oSv:re:iii}{{\dr\upshape(ii)}}]
$\P(\Vabs{\hfedfmpI[(s)]}^2<1/\ssE)\leq4V_{4}(1\wedge \iSv[s]/\ssE)$,
\item[\mylabel{ge:oSv:re:iv}{{\dr\upshape(iii)}}] $\FuEx\Vabs{\fedf[(s)]-\hfedf[(s)]}^2\Vabs{\hfedfmpI[(s)]}^2\leq
  2 (\cst{4} + V_{4})(1\wedge \iSv[s]/\ssE)$.
\end{inparaenum}
\end{lm}

\begin{pro}[Proof of \cref{ge:oSv:re}]
  Since $\ssE\E\Vabs{\fedf(s)-\hfedf(s)}^2=\V[\langle \epsilon_{0} \vert e_{s} \rangle_{\Xi}]\leq V_{4}$ we obtain \ref{ge:oSv:re:ii} as follows
\begin{multline*}
  \E\Vabs{\fedf(s)\hfedfmpI}^2\leq 2\E\{\Vabs{\fedf(s)-\hfedf(s)}^2\Vabs{\hfedfmpI}^2+\Ind{\{|\hfedf(s)|^2\geq1/\ssE\}}\}\\
  \leq 2(\ssE\E(\fedf(s)-\hfedf(s))^2+1) \leq 2 V_{4} + 1.
\end{multline*}

Consider \ref{ge:oSv:re:iii}. Trivially, for any $s\in\Nz$ we
have $\P(|\hfedf(s)|^2<1/\ssE)\leq 1$. If $1\leq
4  V_{4} \ssE^{-1}|\fedf(s)|^{-2})=4V_{4} \ssE^{-1}\iSv[s]$, then obviously
$\P(|\hfedf(s)|^2<\ssE^{-1})\leq\min
(1,4V_{4} \ssE^{-1}\iSv[s])$. Otherwise, we have $\ssE^{-1}<
|\fedf(s)|^2/(4V_{4})$
and hence using Tchebychev's inequaltiy,
\begin{multline*}
\P(|\hfedf(s)|^2<\ssE^{-1})\leq
\P(|\hfedf(s)-\fedf(s)|>|\fedf(s)|/(2\sqrt{V_{4}}))\leq 4\iSv[s]\E\Vabs{\fedf(s)-\hfedf(s)}^2\\\leq4 V_{4} \ssE^{-1} \iSv[s] = \min(1,4V_{4} \ssE^{-1} \iSv[s])  
\end{multline*}
 where we have used again that $\ssE\E\Vabs{\fedf(s)-\hfedf(s)}^2\leq1$. Combining both cases we obtain \ref{ge:oSv:re:iii}.
 
 Consider
 \ref{ge:oSv:re:iv}. Due to \nref{as:il} there is a numerical constant $\cst{4}$ such that
$\ssE^2\E|\fedf(s)-\hfedf(s)|^4\leq \cst{4}$ 
, which in turn implies
\begin{multline*}
 \E\Vabs{\fedf(s)-\hfedf(s)}^2\Vabs{\hfedfmpI}^2\leq \E\Big\{\Vabs{\fedf(s)-\hfedf(s)}^2\Vabs{\hfedfmpI}^22\big[\frac{\Vabs{\fedf(s)-\hfedf(s)}^2}{|\fedf(s)|^2}+\frac{|\hfedf(s)|^2}{|\fedf(s)|^2}\big]\Big\}\\
\leq
\frac{2\ssE\E\Vabs{\fedf(s)-\hfedf(s)}^4}{|\fedf(s)|^2}+\frac{2\E\Vabs{\fedf(s)-\hfedf(s)}^2}{|\fedf(s)|^2}\leq
2(\cst{4} + V_{4}) \ssE^{-1} \iSv[s]. 
\end{multline*}
Combining the last bound and $\E\Vabs{\fedf(s)-\hfedf(s)}^2|\hfedfmpI|^2\leq\ssE\E\Vabs{\fedf(s)-\hfedf(s)}^2\leq V_{4}$  implies \ref{ge:oSv:re:iv},
which completes the proof.
\proEnd
\end{pro}

In addition we will use the following notations.

\begin{nota}
For any $m$ in $\mathds{M}$; $s$ in $\mathds{F}$; and $\theta$ in $\Theta$, let be the following quantities:
\begin{multline*}
\b_{m}^{2}(\theta) := \Vert \theta_{\underline{0}}\Vert_{l^{2}}^{-2} \Vert \theta_{\underline{m}} \Vert^{2}; \quad \Lambda(s) = \vert \lambda^{-1}(s) \vert^{2};\\
\Lambda_{\circ}(m) = m^{-1} \sum\nolimits_{0 < s \leq m} \Lambda(s); \quad \Lambda_{+}(m) := \max\nolimits_{s \in \mathds{F}_{m}}\{\Lambda(s)\}.
\end{multline*}
\assEnd
\end{nota}
Notice that, if $\mathds{F} = \Z$, then $\sum_{s \in \mathds{F}_{m}} \Lambda(s) = 2 m \Lambda_{\circ}(m) + \Lambda(0)$ and if $\mathds{F} = \N^{\star}$ then $\sum_{s \in \mathds{F}_{m}} \Lambda(s) = m \Lambda_{\circ}(m)$.
So in both case we will write $\sum_{s \in \mathds{F}_{m}} \Lambda(s) = \cst{} m \Lambda_{\circ}(m)$.

\begin{ex}{\textsc{Projection estimator} \\}\label{EX_INTRO_FREQ_DECISION_RISKFUNCTION_MSEPROJ}
If one considers a projection estimator, as in \nref{INTRO_FREQ_PROJEST}, one can carry the following computations out for any $m$ in $\mathds{M}$,
\[ \mathcal{R}_{n}(\theta_{n, \overline{m}}, \theta, \Lambda) = \E\left[\Vert \theta_{n, \overline{m}} - \theta \Vert_{l^{2}}^{2} \right] = \sum\nolimits_{s \in \mathds{F}} \V\left[ \theta_{n, \overline{m}}(s) \right] + \vert \E\left[ \theta_{n, \overline{m}}(s) \right] - \theta(s) \vert^{2}.\]

Them, under \nref{as:il}, the quadratic risk can be simplified, depending on the set of assumptions we accept:
\begin{itemize}
\item under \nref{AS_INTRO_DATA_KNOWN} and \nref{AS_INTRO_DATA_INDEPENDENT}
\begin{multline*}
\mathcal{R}_{n}(\theta_{n, \overline{m}}, \theta, \Lambda) = n^{-1} \sum\nolimits_{s \in \mathds{F}_{m}} \Lambda(s) \V\left[ \langle Y_{0} \vert e_{s} \rangle_{L^{2}} \right] + \Vert \theta_{\underline{0}}\Vert_{l^{2}}^{2}\mathfrak{b}_{m}^{2}(\theta)\\
\leq n^{-1} V_{2} \cst{} m \Lambda_{\circ}(s)  + \Vert \theta_{\underline{0}}\Vert_{l^{2}}^{2}\mathfrak{b}_{m}^{2}(\theta) \leq (V_{2} \cst{} + \Vert \theta^{\circ}_{\underline{0}} \Vert_{l^{2}}^{2})[n^{-1} m \Lambda_{\circ}(m) \vee \b_{m}^{2}(\theta^{\circ})];
\end{multline*}
but also
\begin{multline*}
\mathcal{R}_{n}(\theta_{n, \overline{m}}, \theta, \Lambda) \geq n^{-1} V_{1} \cst{} m \Lambda_{\circ}(s)  + \Vert \theta_{\underline{0}}\Vert_{l^{2}}^{2}\mathfrak{b}_{m}^{2}(\theta) \geq (V_{1} \cst{} \vee \Vert \theta^{\circ}_{\underline{0}} \Vert_{l^{2}}^{2})[n^{-1} m \Lambda_{\circ}(m) \vee \b_{m}^{2}(\theta^{\circ})];
\end{multline*}
\item under \nref{AS_INTRO_DATA_KNOWN} and \nref{AS_MARGINS_INTRO_DATA_REGULAR}, using \nref{LMI_INTRO_DATA_REGULAR},
\begin{multline*}
\mathcal{R}_{n}(\theta_{n, \overline{m}}, \theta, \Lambda) = n^{-2} \sum\nolimits_{s \in \mathds{F}_{m}} \Lambda(s) \V\left[\sum\nolimits_{p = 1}^{n} \langle Y_{p} \vert e_{s} \rangle_{L^{2}} \right] + \Vert \theta_{\underline{0}}\Vert_{l^{2}}^{2}\mathfrak{b}_{m}^{2}(\theta)\\
 \leq (\cst{} (1 + 4 \sum\nolimits_{p = 1}^{\infty} \beta(Y_{0}, Y_{p})) + \Vert \theta_{\underline{0}}\Vert_{l^{2}}^{2})[n^{-1} m \Lambda_{\circ}(s) \vee \mathfrak{b}_{m}^{2}(\theta)];
\end{multline*}
\item under \nref{AS_INTRO_DATA_UNKNOWN} and \nref{AS_INTRO_DATA_INDEPENDENT}, start by noticing that, as for any $s$ in $\mathds{F}$, we have $(1 \wedge \Lambda(s)) \leq 1$ and that $\theta^{\circ}$ is square summable, we have, $\sum_{s \in \mathds{F}} \vert \theta^{\circ}(s) \vert^{2} (1 \wedge n_{\lambda}^{-1} \Lambda(s)) < \infty$. Hence, using \nref{ge:oSv:re}, we may write,
\begin{multline*}
  \mathcal{R}_{n, \ssE}(\theta_{n, n_{\lambda}, \overline{m}}, \theta, \Lambda) = \sum\nolimits_{s \in \mathds{F}_{m}} \Lambda(s) \left(\V\left[ \phi_{n}(s) \right] \E\left[ \vert \lambda_{n_{\lambda}}^{+}(s) \lambda(s) \vert^{2} \right]\right) + \Vert \theta_{\underline{0}}\Vert_{l^{2}}^{2}\mathfrak{b}_{m}^{2}(\theta)\\
  + \sum\nolimits_{s \in \mathds{F}_{m}} \vert \theta(s) \vert^{2} \E \left[ \left\vert \lambda_{n_{\lambda}}^{+}(s) \right\vert^{2} \left\vert \lambda(s) - \lambda_{n_{\lambda}}(s)\right\vert^{2} \right] + \sum\nolimits_{s \in \mathds{F}_{m}} \vert \theta(s) \vert^{2} \P( \{\vert \lambda_{n_{\lambda}}(s) \vert^{2} < n_{\lambda}^{-1}\})\\
  \leq (V_{2} \cst{} + \Vert \theta_{\underline{0}}\Vert_{l^{2}}^{2}) [n^{-1} m \Lambda_{\circ}(m) \vee \mathfrak{b}_{m}^{2}(\theta)] + 2 \cst{} (\cst{4} + 3 V_{4}) \sum_{s \in \mathds{F}}\vert \theta(s) \vert^{2} (1 \wedge n_{\lambda}^{-1} \Lambda(s)).
\end{multline*}
\end{itemize}
\remEnd
\end{ex}

\begin{nota}\label{rates}
In particular, we denote in the following way the risk for projection estimators:
\begin{multline*}
\mathcal{R}_{n}^{m}(\theta^{\circ}, \Lambda) := [n^{-1} m \Lambda_{\circ}(m) \vee \b_{m}^{2}(\theta^{\circ})]; \quad \mathcal{R}_{n_{\lambda}}^{\dagger}(\theta^{\circ}, \Lambda) := \sum_{s \in \mathds{F}}\vert \theta(s) \vert^{2} (1 \wedge n_{\lambda}^{-1} \Lambda(s)).
\end{multline*}
\end{nota}
The risk function hence allows us to quantify the performance of an estimator independently of the random observation.
Alternatively, one can consider the probability to exceed a certain loss.

\begin{de}\label{DE_INTRO_FREQ_DECISION_RISKFUNCTION_THRESHOLDOVERCOME}
We define the sequence of functions
\[\mathfrak{R}_{n} : (\mathds{Y} \rightarrow \Xi) \times \Xi \times \R_{+} \rightarrow \R_{+}; \quad (\widehat{f}, f, a) \mapsto \P_{f}^{n}\left(l(\widehat{f}, Y, f) \geq a \right).\]
\assEnd
\end{de}

In general, one is interested in the asymptotic behaviour of $\mathcal{R}$ or $\mathfrak{R}$ (and then replacing $a$ by a sequence $(a_{n})_{n \in \N}$) when $n$ tends to infinity.
In particular, for a given estimator $\widehat{f}$ and a fixed value $f$ of the parameter of interest, the sequence $\mathcal{R}_{n}(\widehat{f}, f)$ is called convergence rate of $\widehat{f}$ at $f$ and if $\mathfrak{R}_{n}(\widehat{f}, f, a_{n})$ tends to $0$ as $n$ tends to infinity, $a_{n}$ is called speed of convergence in probability of $\widehat{f}$ at $f$.
If this sequence tends to zero, the estimator is called consistent.

\medskip

While it is technically feasible to minimise the risk function over $\widehat{f}$ for each $f$, the result will be discountenancing as the minimisers will invariably be functions almost surely equal to $f$ itself which brilliantly yields a loss function equal to $0$, independently of the observation and hence a risk function equal to $0$.
Our goal being to estimate $f$, it is obvious that such an estimator is not at hand.

We are interested in this thesis in two formulations of optimality which allow to overcome this limitation.

\subsubsection{Oracle optimality}\label{INTRO_FREQ_DECISION_ORACLEOPT}
Consider $\mathcal{E}$, a family of estimators and a risk function $\mathcal{R}$.

\begin{de}\label{DE_INTRO_FREQ_DECISION_ORACLEOPT_CONVRATE}
A sequence of functions $\left(\mathcal{R}_{\mathcal{E}, n} : \Xi \rightarrow \R_{+}\right)_{n \in \N}$ is called oracle risk for the family of estimators $\mathcal{E}$ if there exist a constant $C$ in $[1, \infty[$ such that, for any $f$ in $\Xi$, and all $n$, we have: $\mathcal{R}_{\mathcal{E}, n}(f) \leq C \cdot \inf\limits_{\widehat{f} \in \mathcal{E}} \mathcal{R}_{n}(\widehat{f}, f)$, or, depending on the considered set of assumptions, $\mathcal{R}_{\mathcal{E}, n, n_{\lambda}}(f, h) \leq C \cdot \inf\limits_{\widehat{f} \in \mathcal{E}} \mathcal{R}_{n, n_{\lambda}}(\widehat{f}, f, h)$.
\assEnd
\end{de}

\begin{de}\label{DE_INTRO_FREQ_DECISION_ORACLEOPT_EXACTCONVRATE}
A sequence of functions $\mathcal{R}^{\circ}_{\mathcal{E}, n} : \Xi \rightarrow \R_{+}$ is called exact oracle convergence rate for the family of estimators $\mathcal{E}$ if, in addition to being an oracle convergence rate, there exists an element $\widehat{f}$ of $\mathcal{E}$ such that for any $f$ in $\Xi$ and $n$ in $\N$ we have: $\mathcal{R}^{\circ}_{\mathcal{E}, n}(f) \geq C^{-1} \cdot \mathcal{R}_{n}(\widehat{f}, f)$ or $\mathcal{R}^{\circ}_{\mathcal{E}, n, n_{\lambda}}(f, h) \geq C^{-1} \cdot \mathcal{R}_{n, n_{\lambda}}(\widehat{f}, f, h)$ depending on the type of data at hand.
An estimator such as $\widehat{f}$ is called oracle optimal.
\assEnd
\end{de}

We see that those definitions are "up to a constant" and we will in general be more interested in the asymptotic rate as $n$ and/or $n_{\lambda}$ tend to infinity and we hence introduce the following notations.

\begin{nota}
Let be $(a_{n})_{n \in \N}$ a sequence of elements of $\mathds{K}$.
We define the sets $\mathfrak{o}_{n}(a) := \{b \in \mathds{K}^{\N}: \lim_{n \rightarrow \infty}\vert b_{n}/a_{n} \vert = 0\}$; and $\mathcal{O}_{n}(a) := \{b \in \mathds{K}^{\N}: \exists C \in \R_{+} \lim_{n \rightarrow \infty}\vert b_{n}/a_{n} \vert \leq C\}$.
If $a \in \mathcal{O}(b)$ and $b \in \mathcal{O}(a)$ then we denote $a \approx b$.

On the other hand we also define the sets $\mathfrak{o}_{\P}(a)$ and $\mathcal{O}_{\P}(a)$ as the sets of sequences of probability distributions $\P_{n}$ on $\mathds{K}$ such that, if $(X_{n})_{n \in \N}$ is a sequence of $\mathds{K}$-valued random variables verifying $X_{n} \sim \P_{n}$, then we have,
\[ \P_{n} \in \mathfrak{o}_{\P}(a) \iff \forall \epsilon \in \R_{+}^{\star}, \quad \lim_{n \rightarrow \infty} \P(\vert X_{n} / a_{n} \vert \geq \epsilon) = 0\]
\[ \P_{n} \in \mathcal{O}_{\P}(a) \iff \forall \epsilon \in \R_{+}^{\star}, \exists M \in \R_{+}, N \in \N: \quad \forall n > N, \P(\vert X_{n} / a_{n} \vert \geq M) \leq \epsilon\]
\assEnd
\end{nota}

In particular, throughout this thesis, we shall distinguish the following two cases for $\fxdf$, respectively called parametric and non-parametric which commonly lead to very different behaviour of the optimal rates:
\begin{Liste}[]
\item[\mylabel{oo:xdf:p}{\dgrau\bfseries{(p)}}] there exist a finite $K$ of $\mathds{F}$ such that, for any $K'$ smaller than $K$, $\b_{K'}^{2}(\theta^{\circ}) > 0$ and $\b_{K}^{2}(\theta^{\circ}) = 0$;
\item[\mylabel{oo:xdf:np}{\dgrau\bfseries{(np)}}] for all finite $K$ in $\mathds{F}$, $\b_{K}(\theta^{\circ})>0$.
\end{Liste}

Note that the Fourier series expansion of the function of interest $f$ is, in case \ref{oo:xdf:p}, \textit{finite}, i.e., $f=\sum_{s \in \mathds{F}_{K}}\theta^{\circ}(s)e_{s}$ for some finite $K$ in $\N$ while in the opposite case
\ref{oo:xdf:np}, it is \textit{infinite}, i.e., not finite.

\begin{il}\label{IL_INTRO_FREQ_DECISION}
The upper bounds we give will be discussed in such "numerical discussions" where we consider the following typical behaviours of $\fxdf$ and $\fedf$ and give an equivalent to the upper bound in terms of an explicit function of $n$.

Regarding the operator eigen-values $\fedf$, we consider the following two cases, respectively called ordinary smooth and super-smooth:
\begin{Liste}[]
\item[\mylabel{il:edf:o}{\dg\bfseries{(o)}}] there exists a strictly positive real number $a$ such that $\Lambda(m) \approx m^{2a}$, then $m \Lambda_{\circ}(m) \approx m^{2a+1}$ and $\Lambda_{+}(m) \approx m^{2a}$;
\item[\mylabel{il:edf:s}{\dg\bfseries{(s)}}] there exists a strictly positive real number $a$ such that $\Lambda(m) \approx \exp(m^{2a})$, then $m \Lambda_{\circ}(m) \approx m^{-(1-2a)_+}\exp(m^{2a})$ and $\Lambda_{+}(m) \approx \exp(m^{2a})$.
\end{Liste}

For the parameter of interest $\theta^{\circ}$, the behaviours of its tails i.e., $\left(\b_{m}^{2}(\theta^{\circ})\right)_{m \in \mathds{F}} = \Vert \theta^{\circ}_{\underline{0}}\Vert_{l^{2}}^{-2}\Vert \Pi_{\underline{m}} \theta^{\circ} \Vert_{l^{2}}^{2}$ will also be of interest.
We distinguish the cases \ref{oo:xdf:p} and \ref{oo:xdf:np}, and with \ref{oo:xdf:np} distinguish the super smooth and ordinary smooth for the parameter of interest.
\begin{Liste}[]
\item[\mylabel{il:xdf:o}{\dg\bfseries{(o)}}] there exists a strictly positive real number $p$ such that $\vert \theta^{\circ}(s) \vert^{2} \approx s^{-2p -1}$, in this case, we have $\b_{m}^2(\theta^{\circ}) \approx m^{-2p}$;
\item[\mylabel{il:xdf:s}{\dg\bfseries{(s)}}] there exists a strictly positive real number $p$ such that $\vert \theta^{\circ}(s) \vert^{2} \approx s^{2p -1} \exp[-s^{2p}]$, and then we have $\b_{m}^2(\theta^{\circ}) \approx \exp(-m^{2p})$.
\end{Liste}

We consider the following situations: in the cases \begin{inparaenum}[i]
\item[\mylabel{il:po}{\dg\bfseries{[p-o]}}] and \item[\mylabel{il:ps}{\dg\bfseries{[p-s]}}] the parameter of interest has a finite representation \ref{oo:xdf:p} and the operator is either ordinary smooth \ref{il:edf:o} or super smooth \ref{il:edf:s}.
In the cases \item[\mylabel{il:oo}{\dg\bfseries{[o-o]}}] and \item[\mylabel{il:os}{\dg\bfseries{[o-s]}}] the parameter of interest is ordinary smooth \ref{il:xdf:o} and the operator is either ordinary smooth \ref{il:edf:o} or super smooth \ref{il:edf:s}.
Case \item[\mylabel{il:so}{\dg\bfseries{[s-o]}}] is the opposite of case \ref{il:os}.
\end{inparaenum}
\ilEnd
\end{il}

While the names given here to the typical cases may seem arbitrary, we shall justify them through the examples treated in this thesis where the decaying rate of $\fxdf$ and $\fedf$ respectively can be interpreted in terms of function smoothness.

The particular interest for these different cases will also appear natural as the behaviour of the optimal rate will be considerably different in our examples; moreover, this phenomenon is observed in many statistical models, also outside of our field of interest.

We carry on with the projection estimators example.

\textbf{Known operator}

The bound we derived in \nref{rates} depends on the
dimension parameter $\Di$ and hence by selecting an optimal value they
will be minimised, which we formulate next.  For a sequence
$\Nsuite[n]{a_n}$ of real numbers with minimal value in a set
$A\subset{\Nz}$ we set
$\argmin\set{a_n,n\in A}:=\min\{m\in A:a_m\leq a_n,\;\forall n\in A
\}$. For all $n\in\Nz$ we define
\begin{multline}\label{oo:de:nra}
  \dRa{\Di}{\xdf,\Lambda}:=[\bias^2(\xdf)\vee\Di \oiSv \ssY^{-1}]
  :=\max\vectB{\bias^2(\xdf), \Di \oiSv \ssY^{-1}},\\
  \hfill
  \onDi:=\onDi(\xdf,\Lambda):=\argmin\Nset[{\Di\in\Nz}]{\dRa{\Di}{\xdf,\Lambda}}
  \quad\text{ and }\hfill\\
  \oRa{\xdf,\Lambda}:=\dRa{\onDi}{\xdf,\Lambda}=\min\Nset[{\Di\in\Nz}]{\dRa{\Di}{\xdf,\Lambda}}.
\end{multline}

\begin{te}
Consequently, the  rate $\Nsuite[\ssY]{\oRa{\xdf,\Lambda}}$, the dimension parameters $\Nsuite[\ssY]{\onDi}$  and  the projection estimators  $\Nsuite[\ssY]{\txdfPr[\onDi]}$, respectively, is an oracle
rate, an oracle dimension and oracle optimal estimator (up to a constant).
\end{te}

\begin{rmk}\label{RMK_INTRO_FREQ_DECISION_ORACLEOPT_OPTPROJ}\label{oo:rem:ora}
We shall emphasise that $\oRa{\xdf,\Lambda}\geq \ssY^{-1}$ for all
  $\ssY\in\Nz$, and
  
  $\lim_{n \rightarrow \infty} \oRa{\xdf,\Lambda}=0$.
  Observe that for all $\delta>0$ there exists $\Di_{\delta}\in\Nz$ and
  $\ssY_\delta\in\Nz$ such that for all $\ssY\geq \ssY_{\delta}$ holds
  $\bias[\Di_\delta]^2(\xdf)\leq \delta$ and
  $\Di_{\delta} \oiSv[\Di_\delta] \ssY^{-1}\leq\delta$, and whence
  $\oRa{\xdf,\Lambda}\leq\dRa{\Di_\delta}{\xdf,\Lambda}\leq \delta$.
  Moreover, we have $\oDi{\ssY}\in\nset{1,\ssY}$. Indeed, by construction
  holds
  $\bias[\ssY]^2(\xdf)\leq 1<(\ssY+1)\ssY^{-1}\leq
  (\ssY+1)\oiSv[\ssY+1]{}\ssY^{-1}$, and hence
  $\dRa{\ssY}{\xdf,\Lambda}<\dRa{\Di}{\xdf,\Lambda}$ for all
  $\Di\in \nsetro{\ssY+1,\infty}$ which in turn implies the claim
  $\oDi{\ssY}\in\nset{1,\ssY}$. Obviously, it follows thus
  $\oRa{\xdf,\Lambda}=\min\set{ \dRa{\Di}{\xdf,\Lambda}
    ,\Di\in\nset{1,\ssY}}$ for all $\ssY\in\Nz$. We shall use those
  elementary findings in the sequel without further reference.
The sequence $\mathcal{R}_{n}^{\circ}(\theta, \lambda)$ is then an exact oracle convergence rate and the projection estimator $\theta_{n, \overline{m_{n}^{\circ}}}$ is an oracle optimal estimator.
\remEnd
\end{rmk}

\begin{rmk}\label{ge:oo:rem:ora2}
In case \ref{oo:xdf:p}, the oracle rate is parametric, that is
$\oRa{\xdf, \Lambda} \approx \ssY^{-1}$. More precisely, if $\xdf=0$ then
for each  $\Di\in\Nz$,
$\E\Vnormlp{\txdfPr-\xdf}^2=\cst{}\Di\oiSv[\Di]\ssY^{-1}$,
and hence $\oDi{\ssY}=1$ and $\oRa{\xdf, \Lambda}=\oiSv[1]\ssY^{-1}\sim\ssY^{-1}$. Otherwise
if there is $K\in\Nz$  with $\bias[K-1](\xdf)>0$ and
$\bias[K](\xdf)=0$, then setting
$\ssY_{\xdf}:=\tfrac{K\oiSv[K]}{\bias[K-1]^2(\xdf)}$, for all
$\ssY\geq \ssY_{\xdf}$ holds
$\bias[K-1]^2(\xdf)>K\oiSv[K]\ssY^{-1}$, and hence  $\oDi{\ssY}=K$ and
$\oRa{\xdf,\Lambda}= K\oiSv[K]\ssY^{-1}\sim \ssY^{-1}$.
On the other hand side, in case \ref{oo:xdf:np} the oracle rate is
non-parametric, more precisely, it holds
$\lim_{\ssY\to\infty}\ssY\oRa{\xdf,\Lambda}=\infty$. Indeed, since
$\bias[\oDi{\ssY}]^2(\xdf)\leq\oRa{\xdf,\Lambda}=\oRa{\xdf,\Lambda}\in\mathfrak{o}_{n}(1)$ follows $\oDi{\ssY}\to\infty$ and hence
$\oDi{\ssY}\oiSv[\oDi{\ssY}]\to\infty$ which implies the claim because
$\ssY\oRa{\xdf,\Lambda}\geq\oDi{\ssY}\oiSv[\oDi{\ssY}]$.
\end{rmk}

\begin{il}\label{ge:il:oo:kn}
Let us illustrate the rates obtained in the case \ref{oo:xdf:np}.
\begin{Liste}[]
\item[\mylabel{il:oo:oo}{\dg\bfseries{[o-o]}}] 
$\oRa{\xdf,\Lambda}\approx(\oDi{\ssY})^{-2p}\approx (\oDi{\ssY})^{2a+1}\ssY^{-1}$, and hence,
    $\oDi{\ssY}\approx \ssY^{1/(2p+2a+1)}$ and $\oRa{\xdf,\Lambda}\approx\ssY^{-2p/(2p+2a+1)}$
\item[\mylabel{il:oo:os}{\dg\bfseries{[o-s]}}]
$\oRa{\xdf,\Lambda}\approx(\oDi{\ssY})^{-2p}\approx (\oDi{\ssY})^{-(1-2a)_+}\exp((\oDi{\ssY})^{2a})\ssY^{-1}$, and hence,\\
    $\oDi{\ssY}\approx (\log\ssY - \tfrac{2p-(1-2a)_+}{2a}\log\log\ssY)^{1/(2a)}$ and $\oRa{\xdf,\Lambda}\approx(\log\ssY)^{-p/a}$.
\item[\mylabel{il:oo:so}{\dg\bfseries{[s-o]}}] 
$\oRa{\xdf,\Lambda}\approx\exp(-(\oDi{\ssY})^{2p})\approx (\oDi{\ssY})^{2a+1}\ssY^{-1}$, and hence,\\
    $\oDi{\ssY}\approx (\log\ssY - \tfrac{2a+1}{2p}\log\log\ssY)^{1/(2p)}$ and $\oRa{\xdf,\Lambda}\approx(\log\ssY)^{(2a+1)/(2p)}\ssY^{-1}$.
\end{Liste}
\ilEnd
\end{il}

\medskip

\textbf{Unknown operator}

Let us remind that we have
\begin{equation*}
\mathcal{R}_{n, n_{\lambda}}(\theta_{n, n_{\lambda}, \overline{m_{n}^{\circ}}}) \leq (V_{2} \cst{} + \Vert \theta^{\circ}_{\underline{0}} \Vert_{l^{2}}^{2})\mathcal{R}_{n}^{\circ}(\theta^{\circ}, \Lambda) + 2 \cst{} \mathcal{R}_{n_{\lambda}}^{\dagger}(\theta^{\circ}, \Lambda)
\end{equation*}
We note that $\Vnormlp{\xdf_{\underline{0}}}^2=0$ implies
  $\mRa{\xdf,\Lambda}=0$, while for $\Vnormlp{\xdf_{\underline{0}}}^2>0$ holds
  $\mRa{\xdf,\Lambda}\geq \sum_{s:\iSv[s]>\ssE} \vert \fxdf[(s)] \vert ^2+\ssE^{-1}\sum_{s:\iSv[s]\leq\ssE} \vert \fxdf[(s)] \vert ^2  \geq\ssE^{-1}\sum_{s\in\Nz} \vert \fxdf[(s)] \vert ^2=\cst{}\Vnormlp{\xdf_{\underline{0}}}^2
  \ssE^{-1}$, thereby whenever $\xdf\ne 0$
  any additional term of order $\ssY^{-1}+\ssE^{-1}$
  is negligible with respect to the rate
  $\oRa{\xdf,\Lambda}+\mRa{\xdf,\Lambda}$, since
  $\oRa{\xdf,\Lambda}\geq \ssY^{-1}$, 
  which we will use below without further reference. We shall
  emphasise that in case $\ssY=\ssE$ it holds
  \begin{multline}\label{ge:oo:e7}
    \mRa[\ssY]{\xdf,\Lambda}=\sum\nolimits_{s\in \mathds{F}_{m_{n}^{\circ}}} \vert \fxdf[(s)] \vert^2 [1 \wedge \ssY^{-1}\iSv[s]]
    + \sum\nolimits_{s\in \mathds{F}_{m_{n}^{\circ}}^{c}} \vert \fxdf[(s)] \vert ^2[1\wedge\ssY^{-1}\iSv[s]]\\
    \leq \cst{}\Vnormlp{\xdf_{\underline{0}}}^2 \ssY^{-1} \oDi{\ssY}
    \oiSv[\oDi{\ssY}] +
    \cst{}\Vnormlp{\xdf_{\underline{0}}}^2\bias[\oDi{\ssY}]^2(\theta^{\circ})\leq
    \Vnormlp{\xdf_{\underline{0}}}^2\dRa{\oDi{\ssY}}{\xdf,\Lambda}
  \end{multline}
  which in turn implies $\mathcal{R}_{n, n_{\lambda}}(\theta_{n, n_{\lambda}, \overline{m_{n}^{\circ}}}) \leq (V_{2} \cst{} + (1 + 2 \cst{})\Vert \theta^{\circ}_{\underline{0}} \Vert_{l^{2}}^{2})\mathcal{R}_{n}^{\circ}(\theta^{\circ}, \Lambda)$.
  In other words, the estimation of the unknown operator $T$ is negligible whenever $\ssY\leq\ssE$.

\begin{rmk}
We note that in case \ref{oo:xdf:p}
$\mRa{\xdf,\Lambda}\leq
\Vnormlp{\xdf_{\underline{0}}}^2\miSv[K]\ssE^{-1}$
and hence
\begin{equation}\label{oo:au:p}
 \nmRi{\hxdfPr[\oDi{\ssY}]}{\xdf,\Lambda}
 % \FuEx\Vnormlp{\hxdfPr[\oDi{\ssY}]-\xdf}^2
 \leq
\cst{}\{[1\vee\Vnormlp{\xdf_{\underline{0}}}^2]\{
K\oiSv[K]\ssY^{-1}+\miSv[K]\ssE^{-1}\}
\end{equation}
for all $\ssE\in\Nz$ and $\ssY\geq\ssY_{\xdf}$ with $\ssY_{\xdf}$ as in \nref{oo:rem:ora}. In other words the
rate is parametric in both the $\rE$-sample size $\ssE$ and the $\rY$-sample size $\ssY$. Thereby, the  additional estimation of the operator is negligible whenever $\ssE\geq\ssY$.
In the opposite case \ref{oo:xdf:np}, it is obviously of interest to characterise the minimal size $\ssE$ of the additional sample from $\rE$ needed to attain the same rate as in case of a known operator.
Thus, in the next illustration we let the $\rE$-sample size depend on the $\rY$-sample size $\ssY$ as well. 
\remEnd
\end{rmk}

 Let us now briefly illustrate the rates we already defined by stating the
 order of $\oDi{\ssY}(\xdf,\Lambda)$ and $\oRa{\xdf,\Lambda}$ for the cases introduced in \nref{IL_INTRO_FREQ_DECISION}.
% ....................................................................
% <<Il upper bound oo>>
% ....................................................................
\begin{il}\label{ge:il:oo:uk}
\begin{Liste}[]
\item[\mylabel{au:il:oo:oo}{\dg\bfseries{[o-o]}}]
For $p>a$ holds
  $\sum_{s\in\Nz}|\fxdf[(s)]|^2\iSv[s]<\infty$, and hence
  $\moRaE\approx \ssE^{-1}$, while for $p=a$ and $p<a$ holds
  $\sum_{s=1}^{\Di}|\fxdf[(s)]|^2\iSv[s]\approx\log(\Di)$ and
  $\sum_{s=1}^{\Di}|\fxdf[(s)]|^2\iSv[s]\approx\Di^{2(a-p)}$, respectively.
  For $p\leq a$ with $\Di_{\ssE}:=\floor{\ssE^{1/2a}}$ it follows
  $\moRaE\approx\ssE^{-1}\sum_{s\in\nset{1,\Di_{\ssE}}}\iSv[(s)]|\fxdf[(s)]|^2+\bias[\Di_{\ssE}](\xdf)^2$,
  and thereby, 
   $\moRaE\approx\log(\ssE)\ssE^{-1}$ for $p=a$, while 
  $\moRaE\approx\ssE^{-p/a}$ for $p<a$.
\item[\mylabel{au:il:oo:os}{\dg\bfseries{[o-s]}}]
Since
  $\sum_{s=1}^{\Di}|\fxdf[(s)]|^2\iSv[s]\approx\Di^{-2p-1}\iSv[\Di]$  
  the decomposition  in \ref{au:il:oo:oo} with $\Di_{\ssE}:=\floor{(\log\ssE)^{1/(2a)}}$ implies $\moRaE\approx(\log\ssE)^{-p/a}$.
\item[\mylabel{au:il:oo:so}{\dg\bfseries{[s-o]}}] Since
  $\sum_{s\in\Nz}|\fxdf[(s)]|^2\iSv[s]<\infty$ it follows $\moRaE\approx \ssE^{-1}$.
\end{Liste}
\ilEnd
\end{il}

We see that, given a family of estimators, oracle optimality defines the best element of this family.
However, this requires to restrict ourselves to a family of estimator.

\subsubsection{Minimax optimality}\label{INTRO_FREQ_DECISION_MINIMAXOPT}

An alternative to oracle optimality is minimax optimality.

\begin{de}\label{DE_INTRO_FREQ_DECISION_MINIMAXOPT_MAXRATE}
Considering a subset $\widetilde{\Xi}$ of $\Xi$, and an estimator $\widetilde{f}$, we call "maximal convergence rate of $\widetilde{f}$ over $\widetilde{\Xi}$" the sequence indexed by n defined by $\mathcal{R}_{n}(\widetilde{f}, \widetilde{\Xi}, \Lambda) := \sup\limits_{f \in \widetilde{\Xi}} \mathcal{R}_{n}(\widetilde{f}, f).$
Alternatively, if the operator is unknown, we denote $\widetilde{\Xi}$ and $\widetilde{\mathcal{L}(\Xi)}$ two subsets, respectively of $\Xi$ and $\mathcal{L}(\Xi)$ and we have $\mathcal{R}_{n, n_{\lambda}}(\widetilde{f}, \widetilde{\Xi}, \widetilde{\mathcal{L}(\Xi)}) := \sup\limits_{T \in \widetilde{\mathcal{L}(\Xi)}} \sup\limits_{f \in \widetilde{\Xi}} \mathcal{R}_{n, n_{\lambda}}(\widetilde{f}, f, T)$.
\assEnd
\end{de}
We see here that the maximal convergence rate of an estimator corresponds to its worst case scenario over a set of true parameters.
The idea will be to find an estimator with the best worst case scenario.

\begin{de}\label{DE_INTRO_FREQ_DECISION_MINIMAXOPT_OPTRATE}
Considering a subset $\widetilde{\Xi}$ of $\Xi$, a sequence $\mathcal{R}_{n}^{\star}(\widetilde{\Xi}, \Lambda)$ is called minimax convergence rate if there exist a constant $C$ greater than $1$ such that, for any $n$ in $\N$ 
$\mathcal{R}_{n}^{\star}(\widetilde{\Xi}, \Lambda) \leq C \cdot \inf\nolimits_{\widetilde{f} \in \left\{\mathds{Y} \rightarrow \Xi \right\}} \mathcal{R}_{n}(\widetilde{f}, \widetilde{\Xi}, \Lambda)$ where the infimum is taken over all possible estimator.

Moreover, $\mathcal{R}_{n}^{\star}(\widetilde{\Xi}, \Lambda)$ is called minimax optimal convergence rate if there exists some estimator $\widehat{f}$ such that $\mathcal{R}_{n}^{\star}(\widetilde{\Xi}, \Lambda) \geq C^{-1} \cdot \mathcal{R}_{n}(\widehat{f}, \widetilde{\Xi}, \Lambda).$
An estimator such as $\widehat{f}$ is called minimax optimal.
\assEnd
\end{de}
In this definition, be aware that the infimum is taken over all possible estimator of $f$.

An example of space which we use in this thesis as $\widetilde{\Theta}$ are Sobolev's ellipsoids which we already introduced informally previously.
\begin{de}\label{DE_INTRO_FREQ_DECISION_MINIMAXOPT_SOBOL}
Given a constant $r$ in $\R_{+}$, and a positive, decreasing sequence of numbers smaller than $1$, $\left(\mathfrak{a}(s)\right)_{s \in \mathds{F}}$, we define the Sobolev's ellipsoid $\Theta(\mathfrak{a}, r)$ by $\Theta(\mathfrak{a}, r) := \left\{\theta \in \Theta: \Vert \theta \Vert_{\mathfrak{a}} \leq r \right\}$.
\assEnd
\end{de}

Those ellipsoid are interesting as they can directly be related to classes of regularity for the counterpart space $\Xi$.

We now carry on with the projection estimator example.

\textbf{Known operator}

While considering projection estimators, in the case where the operator is known, we may emphasise that for all $\Di\in\Nz^{\star}$ and  any $\xdf\in\rwCxdf$, $\Vnormlp{\xdf_{\underline{0}}}^2\bias^2(\xdf)=\Vnormlp{\xdf_{\underline{\Di}}}^2=\sum_{ \vert s \vert >\Di}(\xdfCw[(s)]^2/\xdfCw[(s)]^2)\fxdf[(s)]^2\leq
\xdfCw[(\Di)]^2\Vnorm[1/{\xdfCw[]}]{\xdf_{\underline{\Di}}}^2\leq
\xdfCw[(\Di)]^2\xdfCr^2$ which we use in the sequel
without further reference.
It follows for all $\Di,\ssY\in\Nz$ that 
  \begin{multline}\label{oo:e4}
\nRi{\txdfPr}{\rwCxdf,\Lambda}:=\sup\set{\nRi{\txdfPr}{\xdf,\Lambda},\xdf\in\rwCxdf}\\
\leq (2+\xdfCr^2)\max\big(\xdfCw^2,\Di \oiSv \ssY^{-1}\big).
\end{multline}
The upper bound in the last display depends on the dimension parameter
$\Di$ and hence by choosing an optimal value $\mnDi$ the upper bound
will be minimised which we formulate next. For all $n\in\Nz$ we define
\begin{multline}
  \label{mm:de:nra}
 \dRa{\Di}{\xdfCw[],\Lambda}:=[\xdfCw^2\vee\Di\oiSv \ssY^{-1}]:=\max\big(\xdfCw^2,\Di \oiSv \ssY^{-1}\big),\\
\hfill \mnDi(\xdfCw[]):=\mnDi(\xdfCw[],\Lambda):=\argmin\Nset[{\Di\in\Nz}]{\dRa{\Di}{\xdfCw[],\Lambda}}\quad\text{ and }\hfill\\\mnRa{\xdfCw[],\Lambda}:=\dRa{\mnDi({\xdfCw[]})}{\xdfCw[],\Lambda}=\min\Nset[{\Di\in\Nz}]{\dRa{\Di}{\xdfCw[],\Lambda}}.
\end{multline}
From \eqref{oo:e4} we deduce that
$\nRi{\txdfPr[\mnDi({\xdfCw[]})]}{\rwCxdf,\Lambda}\leq(2+\xdfCr^2)\mnRa{\xdfCw[],\Lambda}$ for
all $n\in\Nz$. On the other
  hand side, for example, \ncite{JohannesSchwarz2013a} have shown  that
  $\inf_{\widetilde{\theta}}\nRi{\widetilde{\theta}}{\rwCxdf,\Lambda}$, where the infimum is taken over all
  possible estimators $\widetilde{\theta}$ of $\xdf$, is up to a constant bounded
  from below by $\mnRa{\xdfCw[],\Lambda}$.  Consequently, the rate
  $\Nsuite[n]{\mnRa{\xdfCw[],\Lambda}}$, the dimension parameters $\Nsuite[n]{\mnDi(\xdfCw[])}$
  and the projection estimators $\Nsuite[n]{\txdfPr[\mnDi({\xdfCw[]})]}$, respectively, is a
  minimax rate, a minimax dimension and minimax optimal (up to a
  constant).

% ....................................................................
% Bemerkung mm
% ....................................................................
\begin{rmk}\label{oo:rem:mra}
By construction it holds 
$\mnRa{\xdfCw[],\Lambda}\geq \ssY^{-1}$ for all $\ssY\in\Nz$.
The following statements can be
shown using the same arguments as in \nref{oo:rem:ora}
by exploiting that the sequence $\xdfCw[]$ is assumed to be
non-increasing, strictly positive with limit zero and $\xdfCw[(1)]=1$. 
Thereby, we conclude that 
$\mnRa{\xdfCw[],\Lambda}=\mathfrak{o}_{n}(1)$ and $\ssY\mnRa{\xdfCw[],\Lambda}\to\infty$ as well as 
$\mnDi(\xdfCw[])\in\nset{1,\ssY}$ for all $\ssY\in\Nz$. It follows also that
$\mnDi(\xdfCw[])=\argmin\Nset[{\Di\in\nset{1,n}}]{\dRa{\Di}{\xdfCw[],\Lambda}}$ and 
$\mnRa{\xdfCw[],\Lambda}=\min\Nset[{\Di\in\nset{1,n}}]{\dRa{\Di}{\xdfCw[],\Lambda}}$ for all
$\ssY\in\Nz$. We shall stress that in this situation the rate
$\mnRa{\xdfCw[],\Lambda}$ is non-parametric. \remEnd
\end{rmk}


Let us now briefly illustrate the last definitions by stating the
order of $\mnDi(\xdfCw[],\Lambda)$ and $\mnRa{\xdfCw[],\Lambda}$ for typical choices of the
sequence $\xdfCw[]$.

% ....................................................................
% <<Il upper bound oo>>
% ....................................................................
\begin{il}\label{il:mm}We will illustrate all our results considering
  the following two configurations for the sequence $\xdfCw[]$. Let 
\begin{Liste}[]
\item[\mylabel{il:mm:o}{\dg\bfseries{(o)}}] $\xdfCw^2\approx
  \Di^{-2p}$ with $p>1$;
\item[\mylabel{il:mm:s}{\dg\bfseries{(s)}}] $\xdfCw^2\approx
  \exp(-\Di^{2p})$ with $p>0$.
\end{Liste}
We consider as in \nref{il:oo} the situations \ref{il:oo}, \ref{il:os}
and \ref{il:so}. 
\begin{Liste}[]
\item[\mylabel{il:mm:oo}{\dg\bfseries{[o-o]}}] 
$\dRa{\mnDi}{\xdfCw[],\Lambda}\approx(\mnDi)^{-2p}\approx (\mnDi)^{2a+1}\ssY^{-1}$, and hence,

    $\mnDi({\xdfCw[]})\approx \ssY^{1/(2p+2a+1)}$ and $\mnRa{\xdfCw[],\Lambda}\approx\ssY^{-2p/(2p+2a+1)}$
\item[\mylabel{il:mm:os}{\dg\bfseries{[o-s]}}]
$\dRa{\mnDi}{\xdfCw[],\Lambda}\approx(\mnDi)^{-2p}\approx (\mnDi)^{-(1-2a)_+}\exp((\mnDi)^{2a})\ssY^{-1}$, and hence,\\
    $\mnDi({\xdfCw[]})\approx (\log\ssY - \tfrac{2p-(1-2a)_+}{2a}\log\log\ssY)^{1/(2a)}$ and $\mnRa{\xdfCw[],\Lambda}\approx(\log\ssY)^{-p/a}$.
\item[\mylabel{il:mm:so}{\dg\bfseries{[s-o]}}] 
$\dRa{\mnDi}{\xdfCw[],\Lambda}\approx\exp(-(\mnDi)^{2p})\approx (\mnDi)^{2a+1}\ssY^{-1}$, and hence,\\
    $\mnDi({\xdfCw[]})\approx (\log\ssY - \tfrac{2a+1}{2p}\log\log\ssY)^{1/(2p)}$ and $\mnRa{\xdfCw[],\Lambda}\approx(\log\ssY)^{(2a+1)/(2p)}\ssY^{-1}$.\ilEnd
\end{Liste}
\end{il}

\medskip

\textbf{Unknown operator}

Consider now the case where the operator is unknown.
For all $\ssE\in\Nz$ we define
\begin{equation}\label{oo:de:mra}
  \mmRa{\xdfCw[],\Lambda}:=\max_{s\in\Nz}\{\xdfCw[(s)]^2[1\wedge \iSv[s]/\ssE]\}.
\end{equation}
then for all $\ssE\in\Nz$ holds
$\sup_{\xdf\in\rwCxdf}\mmRa{\xdf,\Lambda}\leq
\xdfCr^2\mmRa{\xdfCw[],\Lambda}$, since for all
$\xdf\in\rwCxdf$ 
\begin{equation}\label{oo:e8}
  \mmRa{\xdf,\Lambda}=\sum_{s\in\Nz} \vert \fxdf[(s)] \vert ^2[1\wedge \iSv[s]/\ssE]\leq
\max_{s\in\Nz}\{\xdfCw[(s)]^2\min(1,\iSv[s]/\ssE)\}\Vnorm[1/{\xdfCw[]}]{\xdf}^2.
\end{equation}
It follows for all $\Di,\ssY,\ssE\in\Nz$ immediately that 
\begin{equation}\label{oo:e9}
  \nmRi{\hxdfPr}{\rwCxdf,\Lambda}
  \leq (\xdfCr^2+8) \dRa{\Di}{\xdfCw[],\Lambda}+8(\cst{4}+1)\xdfCr^2\mmRa{\xdfCw[],\Lambda}.
\end{equation}
The upper bound in the last display depends on the dimension parameter
$\Di$ and hence by choosing an optimal value $\mnDi$ as in
\eqref{mm:de:nra} the upper bound
will be minimised, that is
\begin{equation}\label{oo:e10}
  \nmRi{\hxdfPr[\mnDi]}{\rwCxdf,\Lambda}
  \leq (\xdfCr^2+8) \mnRa{\xdfCw[],\Lambda}+8(\cst{4}+1)\xdfCr^2\mmRa{\xdfCw[],\Lambda}.
\end{equation}

% ....................................................................
% <<Il upper bound oo>>
% ....................................................................
\begin{il}\label{au:il:mm}
  Consider as in \nref{il:mm} the usual
  behaviours \ref{il:mm:oo}, \ref{il:mm:os} and \ref{il:mm:so} for the
  sequences $\Nsuite[\Di]{\xdfCw}$ and $\Nsuite[\Di]{\iSv[\Di]}$,
  where we have derived in  \nref{il:mm} the corresponding minimax
  rates $\Nsuite[\ssY]{\mnRa{\xdfCw[],\Lambda}}$, while for  the rate
  $\Nsuite[\ssE]{\mmRa{\xdfCw[],\Lambda}}$ we get:
\begin{Liste}[]
\item[\mylabel{au:il:mm:oo}{\dg\bfseries{[o-o]}}]  
$\mmRa{\xdfCw[],\Lambda}\approx\ssE^{-(p\wedge a)/a}$
\item[\mylabel{au:il:mm:os}{\dg\bfseries{[o-s]}}]
 $\mmRa{\xdfCw[],\Lambda}\approx(\log\ssE)^{-p/a}$.
\item[\mylabel{au:il:mm:so}{\dg\bfseries{[s-o]}}] 
 $\mmRa{\xdfCw[],\Lambda}\approx \ssE^{-1}$.\ilEnd
\end{Liste}
\end{il}

\begin{rmk} Since the operator $T$ is not known, it is natural to
  consider a maximal risk also over a class for $\edf$ characterising the behaviour of
  $\Nsuite[s]{\iSv[s]= \vert \fedf[(s)] \vert ^{-2}}$, precisely $\rwCedf:=\{\edf\in\lp[2]:\edfCr^{-2}\leq\edfCw[s] \vert \fedf[ \vert s \vert ] \vert ^{2}=
\edfCw[s]/\iSv[ \vert s \vert ]\leq \edfCr^{2},\;\forall s\in\Nz\}\cap\cD$.
We shall note that for all $\Di\in\Nz$ and any $\edf\in\rwCedf$,
$\edfCr^{-2}\leq\miSv/\edfCwm\leq \edfCr^{2}$,
$\edfCr^{-2}\leq\oiSv/\edfCwo\leq \edfCr^{2}$. Setting
for all $\ssY,\ssE\in\Nz$
\begin{multline}\label{mm:de:mnra}
 \dRa{\Di}{\xdfCw[],\edfCw[]}:=[\xdfCw^2\vee\Di\edfCwo \ssY^{-1}],
\hfill
\mnDi(\xdfCw[],\edfCw[]):=\argmin\Nset[{\Di\in\Nz}]{\dRa{\Di}{\xdfCw[],\edfCw[]}},\hfill\\\mnRa{\xdfCw[],\edfCw[]}:=\dRa{\mnDi({\xdfCw[],\edfCw[]})}{\xdfCw[],\edfCw[]}=\min\Nset[{\Di\in\Nz}]{\dRa{\Di}{\xdfCw[],\edfCw[]}}\quad\text{
  and }\\
\mnRa{\xdfCw[],\edfCw[]}:=\max\{\xdfCw[(s)]\min(1,\edfCw[s]/\ssE),s\in\Nz\}.
\end{multline}
we have 
\begin{multline}\label{oo:e11}
 \mnRa{\xdfCw[],\Lambda}=\min_{\Di\in\Nz}\{[\xdfCw\vee\Di\oiSv\ssY^{-1}]\}\leq
 \edfCr^2\min_{\Di\in\Nz}\{[\xdfCw\vee\Di\edfCwo\ssY^{-1}]\}\leq \edfCr^2\dRa{\Di}{\xdfCw[],\edfCw[]}\\ 
  \mmRa{\xdfCw[],\Lambda}=\max_{s\in\Nz}\{\xdfCw[(s)]^2[1\wedge \iSv[s]/\ssE]\}\leq\edfCr^2\mnRa{\xdfCw[],\edfCw[]}.
\end{multline}
It follows for all $\Di,\ssY\in\Nz$ immediately that 
\begin{equation}\label{oo:e12}
  \nmRi{\hxdfPr}{\rwCxdf,\rwCedf}
  \leq (\xdfCr^2+8\edfCr^2) \mnRa{\xdfCw[],\edfCw[]}
+8(\cst{4}+1)\edfCr^2\xdfCr^2\mmRa{\xdfCw[],\edfCw[]}.
\end{equation}
\ncite{JohannesSchwarz2013a} have shown  that
  $\inf_{\hxdf}\nmRi{\hxdf}{\rwCxdf,\rwCedf}$, where the infimum is taken over all
  possible estimators $\hxdf$ of $\xdf$, is up to a constant bounded
  from below by $\mnRa{\xdfCw[],\edfCw[]}\vee\mmRa{\xdfCw[],\edfCw[]} $.  Consequently, the rate
  $\Nsuite[n]{\mnRa{\xdfCw[],\edfCw[]}\vee\mmRa{\xdfCw[],\edfCw[]}}$, the dimension parameters $\Nsuite[n]{\mnDi(\xdfCw[])}$
  and the projection estimators $\Nsuite[n]{\txdfPr[\mnDi({\xdfCw[]})]}$, respectively, is a
  minimax rate, a minimax dimension and minimax optimal (up to a
  constant).
\remEnd
\end{rmk}

%\subsection{Adaptivity}\label{INTRO_FREQ_ADAPTIVITY}
%We have given in the previous subsection criteria allowing to justify the choice of an estimator.
%However, we have also seen, while considering the example of projection estimators, that satisfying estimators often rely on some knowledge of the true parameter, which is in practice not known, in order to chose a tuning parameter.
%Hence, developing adaptive methods (that is to say, methods which do not rely on such knowledge) is mandatory for practical use and many technics have been considered in the past.
%We briefly present here the penalised contrast model selection technic, presented in more details, for example in \ncite{barron1999risk}, or \ncite{comte2006penalized}, and for which we will give a new interpretation in the following chapters as well as a new technic to obtain optimality results.
%
%Let be a sieve family such as $(\mathds{U}_{\overline{m}})_{m \in \mathds{M}}$, and the naive estimator $\theta_{n}$.
%One can notice that for any $x$ in $\mathds{U}_{\overline{m}}$, we have $\Vert x - \theta^{\circ} \Vert_{l^{2}}^{2} = \Vert x - \theta_{n} + \theta_{n} - \theta^{\circ} \Vert_{l^{2}}^{2} = \Vert x - \theta_{n} \Vert_{l^{2}}^{2}  - 2 \langle x - \theta_{n} \vert \theta^{\circ}_{\overline{m}} - \theta_{n} \rangle_{l^{2}} + \Vert \theta^{\circ} - \theta_{n} \Vert_{l^{2}}^{2} = \Vert x - \theta_{n} \Vert_{l^{2}}^{2}  - 2 (\langle x \vert \theta^{\circ} \rangle_{l^{2}} - \langle x \vert \theta_{n} \rangle_{l^{2}} - \langle \theta_{n} \vert \theta^{\circ} \rangle_{l^{2}} + \langle \theta_{n} \vert \theta_{n} \rangle_{l^{2}}) + \Vert \theta^{\circ} - \theta_{n} \Vert_{l^{2}}^{2}$.
%Hence, one would define the so called contrast function $\gamma$ from $\Theta_{\overline{m}}$ to $\R$ which to $x$ associates $\gamma(x) = \Vert x \Vert_{l^{2}}^{2} - 2 \langle x \vert \theta^{\circ}_{\overline{m}} \rangle_{l^{2}}$ and the associated contrast minimising estimator, if it exists $x_{n, m} = \argmin_{x \in \Theta_{\overline{m}}} \{\gamma(x)\}$.