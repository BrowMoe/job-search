\documentclass[10pt]{beamer}
\usetheme[
%%% option passed to the outer theme
%    progressstyle=fixedCircCnt,   % fixedCircCnt, movingCircCnt (moving is deault)
  ]{Feather}
  
% If you want to change the colors of the various elements in the theme, edit and uncomment the following lines

% Change the bar colors:
\setbeamercolor{Feather}{fg=red!80!black,bg=red!60!black}

% Change the color of the structural elements:
\setbeamercolor{structure}{fg=black}

% Change the frame title text color:
\setbeamercolor{frametitle}{fg=yellow}

% Change the normal text color background:
%\setbeamercolor{normal text}{fg=black,bg=gray!10}

%-------------------------------------------------------
% INCLUDE PACKAGES
%-------------------------------------------------------
\usepackage{ifpdf}
\usepackage{graphicx} 
\usepackage{caption}
\usepackage{subcaption}
\ifpdf
\DeclareGraphicsRule{*}{mps}{*}{}
\else
% Recent LaTeX versions donât require the next line
% \DeclareGraphicsRule{*}{eps}{*}{}
\fi

\usepackage[english, french]{babel}
\usepackage[utf8]{inputenc}
%\usepackage[TS1,T1]{fontenc}
%\usepackage{helvet}
\usepackage{fourier}
%\usepackage{hyperref}
%\usepackage{lmodern}
%\usepackage{mathrsfs}
\usepackage{natbib}
\usepackage{amsmath,amssymb,amsfonts,graphicx,shorttoc,textpos,caption,here, yfonts}
\usepackage{verbatim,enumerate,dsfont,fancyhdr,setspace,array}

%\usepackage[style=authoryear]{biblatex}
%\renewcommand*{\nameyeardelim}{\addcomma\addspace}


\graphicspath{{Feathergraphics/}}
\bibliography{biblio}
%\DeclareCiteCommand{\cite}
%{\usebibmacro{prenote}}
%{\usebibmacro{citeindex}%
%\usebibmacro{cite}}
%{\multicitedelim}
%{\usebibmacro{cite:postnote}}
%\usefonttheme[onlymath]{serif}
%\newbibmacro*{cite}{%
%\usebibmacro{cite:citepages}%
%\ifciteseen
%	{\iffieldundef{shorthand}
%		{\usebibmacro{cite:short}}
%		{\usebibmacro{cite:shorthand}}}
%	{\usebibmacro{cite:full}}}
%
%\renewbibmacro*{cite}{%
%\usebibmacro{cite:citepages}%
%{\usebibmacro{cite:full}}}
%-------------------------------------------------------
% DEFFINING AND REDEFINING COMMANDS
%-------------------------------------------------------

% colored hyperlinks
\newcommand{\chref}[2]{
  \href{#1}{{\usebeamercolor[bg]{Feather}#2}}
}
\DeclareMathOperator*{\argmin}{arg\,min}
%\bibliographystyle{apalike}
%\renewcommand\bibfont{\scriptsize}
\DeclareMathOperator*{\Argmin}{Arg\,min}
\setbeamertemplate{frametitle continuation}[from second]

%-------------------------------------------------------
% INFORMATION IN THE TITLE PAGE
%-------------------------------------------------------

\title[A Bayesian interpretation of data-driven estimation by model selection] % [] is optional - is placed on the bottom of the sidebar on every slide
{ % is placed on the title page
     \textbf{A Bayesian interpretation of data-driven estimation by model selection}
}






\institute[Ruprecht-Karls-Universität Heidelberg]
{
  
  Ruprecht-Karls-Universität Heidelberg
  %there must be an empty line above this line - otherwise some unwanted space is added between the university and the country (I do not know why;( )
}

\date{In collaboration with Jan Johannes}

\author[Xavier Loizeau] % [] is optional - is placed on the bottom of the sidebar on every slide
{ % is placed on the title page
      \textit{Xavier Loizeau\\      $11^{th}$ of December 2016}
}
%-------------------------------------------------------
% THE BODY OF THE PRESENTATION
%-------------------------------------------------------

\begin{document}
%-------------------------------------------------------
% THE TITLEPAGE
%-------------------------------------------------------

{\1% % this is the name of the PDF file for the background
\begin{frame}[plain,noframenumbering] % the plain option removes the header from the title page, noframenumbering removes the numbering of this frame only
  \titlepage % call the title page information from above
\end{frame}}

\AtBeginSection[]{
\begin{frame}{Contents}
\setcounter{tocdepth}{2}
\tableofcontents[sectionstyle=show/shaded, subsectionstyle=show/show/hide]
\end{frame}
}

\begin{frame}{Contents}
\setcounter{tocdepth}{1}
\tableofcontents
\end{frame}

%-------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------
\subsection{Considered model}

\begin{frame}{Considered model}{Indirect sequence space model}

Consider an indirect Gaussian sequence space model consisting of:
\begin{itemize}
\item<2-> an unknown parameter of interest $\left(\theta^{\circ}_{j}\right)_{j \in \mathbb{N}} = \theta^{\circ}$,
\item<3-> a decreasing multiplicative sequence $\left(\lambda_{j}\right)_{j \in \mathbb{N}} = \lambda$ converging to $0$,
\item<4-> observations $\left(Y_{j}\right)_{j \in \mathbb{N}} = Y$, contaminated by an additive independent centered Gaussian noise with variance $n^{-1}$,
\end{itemize}

\bigskip

\textcolor{red!90!black}{\[Y = \left(\theta^{\circ}_{j} \cdot \lambda_{j} + \sqrt{n}^{-1} \cdot \xi_{j}\right)_{j \in \mathbb{N}}, \quad \left(\xi_{j}\right)_{j \in \mathbb{N}} \sim_{iid} \mathcal{N}\left(0, 1\right).\]}

The goal is to recover $\theta^{\circ}$ and derive an upper bound.
\end{frame}

\begin{frame}{Considered model}{Background}
A motivation for the model is the \textcolor{red!90!black}{Backwards heat equation}.

%See for example \citet{HEMHAN}.
\end{frame}

\subsection{A popular frequentist method}
\begin{frame}{A popular frequentist method}{Projection estimators}
From a frequentist point of view, a natural method to answer this problem is:
\begin{itemize}
\item<1-> for any $j$ in $\mathbb{N}$, consider the unbiased estimator $\widetilde{\theta}_{j}$ of $\theta^{\circ}_{j}$: $\textcolor{red!90!black}{\widetilde{\theta}_{j} = \frac{Y_{j}}{\lambda_{j}}};$
\item<2-> define the family of projection estimators of $\theta^{\circ}$: $\left\{\left(\widetilde{\theta}^{m}_{j}\right)_{j \in \mathbb{N}} : \forall j \in \mathbb{N}, \widetilde{\theta}^{m}_{j} = \widetilde{\theta}_{j} \mathds{1}_{\left\{j \leq m\right\}} \right\};$
\item<3-> define a method to select $m$ in a satisfactory way.
\end{itemize}
\end{frame}

\begin{frame}{A popular frequentist method}{Model selection}
Model selection by penalized contrast is a method operating such selection:
\begin{itemize}
\item<1-> define for any $G_{n} := \max\left\{j \in \llbracket 1, n \rrbracket : n^{-1} \lambda_{j}^{-2} \leq \lambda_{1}^{-2}\right\},$
\item<2-> for any $m$ in $\llbracket 1, G_{n}\rrbracket$ define $pen(m) := 3 \cdot m,$
\item<3-> for any $m$ in $\llbracket 1, G_{n}\rrbracket$ define $\gamma(m) := -\sum\limits_{j = 1}^{m} Y_{j}^{2},$
\item<4-> define $\textcolor{red!90!black}{\widehat{m} := \argmin\limits_{m \in \llbracket 1, G_{n}\rrbracket}\left\{pen(m) + \gamma(m)\right\}}.$
\end{itemize}

the frequentist estimator defined this way is $\textcolor{red!90!black}{\widehat{\theta} = \left(\widetilde{\theta}^{\widehat{m}}_{j}\right)_{j \in \mathbb{N}}}.$

It is shown in \citet{PM}, in the direct case, that this estimator is \textcolor{red!90!black}{consistent}, converges in probability and $\mathbb{L}^{2}$-norm, noted $\Vert \cdot \Vert$, with \textcolor{red!90!black}{minimax optimal rate} over some Sobolev ellipsoid:
\[\textcolor{red}{\Theta^{\circ} := \Theta^{\circ}\left(\textgoth{a}, L^{\circ}\right) \left\{\theta : \sum\limits_{j = 1}^{\infty} \frac{1}{\textgoth{a}_{j}}\theta_{j}^{2} < L^{\circ}\right\}}.\]
\end{frame}

\subsection{Bayesian point of view}
\begin{frame}{Bayesian point of view}{Bayesian fondamental paradigm}
The problem is here treated from a Bayesian point of view:
\begin{itemize}
\item<1-> the parameter $\boldsymbol{\theta}$ is a random variable with \textcolor{red!90!black}{prior} $\textcolor{red!90!black}{\mathbb{P}_{\boldsymbol{\theta}}},$
\item<2-> given $\boldsymbol{\theta},$ the \textcolor{red!90!black}{likelihood} of $Y$ is $\textcolor{red!90!black}{\mathbb{P}_{Y \vert \boldsymbol{\theta}}^{n}} = \mathcal{N}\left(\boldsymbol{\theta} \cdot \lambda, n^{-1} \mathbb{I}\right),$
\item<3-> we are interested in the \textcolor{red!90!black}{posterior distribution} $\textcolor{red!90!black}{\mathbb{P}_{\boldsymbol{\theta} \vert Y}^{n}} \propto \mathbb{P}_{Y \vert \boldsymbol{\theta}}^{n} \cdot \mathbb{P}_{\boldsymbol{\theta}}.$
\end{itemize}
\end{frame}

\begin{frame}{Bayesian point of view}{Iterated posterior distribution}
In the spirit of \citet{OBJJ}, we then generate a posterior family by introducing an \textcolor{red!90!black}{iteration parameter $\eta$}:
\begin{itemize}
\item<1-> for $\textcolor{red!90!black}{\eta = 1}$, the prior distribution is $\mathbb{P}_{\boldsymbol{\theta}^{1}} = \mathbb{P}_{\boldsymbol{\theta}},$ the likelihood $\mathbb{P}_{Y^{1} \vert \boldsymbol{\theta}^{1}}^{n} = \mathbb{P}_{Y \vert \boldsymbol{\theta}}^{n}$ and the posterior distribution is $\mathbb{P}_{\boldsymbol{\theta}^{1} \vert Y^{1}}^{n} = \mathbb{P}_{\boldsymbol{\theta}\vert Y}^{n}.$
\item<2-> for $\textcolor{red!90!black}{\eta = 2},$ we take the posterior for $\eta = 1$ as prior, hence, the \textcolor{red!90!black}{prior is} $\textcolor{red!90!black}{\mathbb{P}_{\boldsymbol{\theta}^{2}}^{n} = \mathbb{P}_{\boldsymbol{\theta}^{1} \vert Y^{1}}^{n}},$ the likelihood does not change $\mathbb{P}_{Y^{2} \vert \boldsymbol{\theta}^{2}}^{n} = \mathbb{P}_{Y \vert \boldsymbol{\theta}}^{n}$ and we compute the posterior with the same observations $Y$, which we note $\mathbb{P}_{\boldsymbol{\theta}^{2} \vert Y^{2}}^{n}.$
\item<3-> ...
\item<4-> for any value of $\textcolor{red!90!black}{\eta > 1},$ the prior is $\textcolor{red!90!black}{\mathbb{P}_{\boldsymbol{\theta}^{\eta}}^{n} = \mathbb{P}_{\boldsymbol{\theta}^{\eta-1} \vert Y^{\eta - 1}}}$ and we compute the posterior with the same likelihood $\textcolor{red!90!black}{\mathbb{P}_{Y^{\eta} \vert \boldsymbol{\theta}^{\eta}}^{n} = \mathbb{P}_{Y \vert \boldsymbol{\theta}}^{n}}$ and same observations $Y$ which gives $\textcolor{red!90!black}{\mathbb{P}_{\boldsymbol{\theta}^{\eta} \vert Y^{\eta}}^{n}}.$
\end{itemize}
\end{frame}

\subsection{Considered questions}
\begin{frame}{Considered questions}
\begin{itemize}
\item Interpretation: giving more and more weight to observations, making prior knowledge vanish;
\item generating a family of Bayes estimates $\textcolor{red!90!black}{\widehat{\theta}^{\eta}} := \mathbb{E}_{\boldsymbol{\theta}^{\eta} \vert Y^{\eta}}^{n} \left[\boldsymbol{\theta}\right];$
\item for any $\eta,$ study the behavior of $\mathbb{P}_{\boldsymbol{\theta}^{\eta} \vert Y^{\eta}}^{n}$ and $\widehat{\theta}^{\eta}$ as $\textcolor{red!90!black}{n \rightarrow \infty};$
\item give particular attention to the \textcolor{red!90!black}{"selfinformative limit"} $\lim\limits_{\eta \rightarrow \infty} \widehat{\theta}^{\eta}$ and the \textcolor{red!90!black}{"selfinformative Bayes carrier"}  $\lim\limits_{\eta \rightarrow \infty} \mathbb{P}_{\boldsymbol{\theta}^{\eta} \vert Y^{\eta}}^{n}$
\end{itemize}

In particular, the question of \textcolor{red!90!black}{oracle and minimax} concentration (resp. convergence) is answered for any element of the family of posterior distribution (resp. posterior means), including when $\eta$ tends to infinite.

\end{frame}

\section{Suggested method}
\subsection{Hierarchical priors}

\begin{frame}{Hierarchical prior}{Sieve prior}
\begin{itemize}
\item Consider a \textcolor{red!90!black}{threshold parameter} $m$, depending on $n$:
\begin{align*}
\forall j > m ,& \quad \mathbb{P}_{\boldsymbol{\theta}_{j}} = \delta_{0},\\
\forall j \leq m ,& \quad \mathbb{P}_{\boldsymbol{\theta}_{j}} = \mathcal{N}\left(0, 1\right).
\end{align*}
\item Hence, the posterior is
\begin{align*}
\forall j > m, &\quad \boldsymbol{\theta}_{j} \vert M = m, Y \sim \delta_{0},\\
\forall j \leq m, &\quad \boldsymbol{\theta}_{j} \vert M = m, Y \sim \mathcal{N}\left(\frac{Y_{j} \cdot n \cdot \lambda_{j}}{1 + n \cdot \lambda_{j}^{2}}, \frac{1}{1 + n \cdot \lambda_{j}^{2}} \right).
\end{align*}
\end{itemize}

\underline{Problem:} $m$ has to be chosen; if $\theta^{\circ}$ or $\Theta^{\circ}$ is known, then it can be chosen optimally for each $n$ for a certain criterion.

\end{frame}

\begin{frame}{Hierarchical prior}
\begin{itemize}
\item Consider a \textcolor{red!90!black}{random hyper-parameter} \textcolor{red!90!black}{$M$}, with values in $\llbracket 1, G_{n} \rrbracket$, acting like a threshold:
\begin{align*}
\forall j > m ,& \quad \mathbb{P}_{\boldsymbol{\theta}_{j}\vert M = m} = \delta_{0},\\
\forall j \leq m ,& \quad \mathbb{P}_{\boldsymbol{\theta}_{j}\vert M = m} = \mathcal{N}\left(0, 1\right).
\end{align*}
\item if we denote $\textcolor{red!90!black}{\mathbb{P}_{M}}$ the distribution of $M$ (to be specified later), then
\[\textcolor{red!90!black}{\mathbb{P}_{\boldsymbol{\theta} \vert Y}^{n} = \sum\limits_{m \in \mathbb{N}} \mathbb{P}_{\boldsymbol{\theta} \vert M = m, Y} \cdot \mathbb{P}_{M = m \vert Y}} .\]
\item Hence, given $M$, the posterior is
\begin{align*}
\forall j > m, &\quad \boldsymbol{\theta}_{j} \vert M = m, Y \sim \delta_{0},\\
\forall j \leq m, &\quad \boldsymbol{\theta}_{j} \vert M = m, Y \sim \mathcal{N}\left(\frac{Y_{j} \cdot n \cdot \lambda_{j}}{1 + n \cdot \lambda_{j}^{2}}, \frac{1}{1 + n \cdot \lambda_{j}^{2}} \right).
\end{align*}
\end{itemize}
\end{frame}

\subsection{Explicit formulation}
\begin{frame}{Explicit formulation}{Conditional posterior}
The iterated posterior for a hierarchical prior can be written
\textcolor{red!90!black}{
\begin{alignat*}{2}
&\mathbb{P}_{\boldsymbol{\theta}^{\eta}\vert Y^{\eta}}^{n} &&= \sum\limits_{m \in \mathbb{N}} \mathbb{P}_{\boldsymbol{\theta}^{\eta} \vert M^{\eta} = m, Y^{\eta}}^{n} \cdot \mathbb{P}^{n}_{M^{\eta} = m \vert Y^{\eta}},\\
&\widehat{\theta}^{\left(\eta\right)} &&= \left(\mathbb{E}_{\boldsymbol{\theta}^{\eta}\vert M^{\eta} \geq j, Y^{\eta}}^{n}\left[\boldsymbol{\theta}_{j}\right] \cdot \mathbb{P}_{M^{\eta} \vert Y^{\eta}}^{n}\left(M^{\eta} \geq j\right)\right)_{j \in \mathbb{N}}.
\end{alignat*}}

Hence, we first compute $\boldsymbol{\theta}^{\eta}_{j} \vert M^{\eta}, Y^{\eta}$:
\textcolor{red!90!black}{\begin{alignat*}{4}
& \forall j \in \mathbb{N}, && \quad \boldsymbol{\theta}^{\eta}_{j} \vert M^{\eta} \geq j, Y^{\eta} &&\sim &&\mathcal{N}\left(\frac{\eta \cdot Y_{j} \cdot n \cdot \lambda_{j}}{1 + \eta \cdot n \cdot \lambda_{j}^{2}}, \frac{1}{1 + n \cdot \eta \cdot \lambda_{j}^{2}} \right),\\
&  && \quad \boldsymbol{\theta}^{\eta}_{j} \vert M^{\eta} < j, Y^{\eta} &&\sim &&\delta_{0}.
\end{alignat*}}
\end{frame}

\begin{frame}{Explicit formulation}{Graphic representation}
\begin{figure}
\centering
 \includegraphics[width=.8\linewidth]{M.pdf}
\caption{Survival function of $M$ for different values of $n$}\label{M}
\end{figure}
\end{frame}

\begin{frame}{Explicit formulation}{Iterated posterior}
We then fix the distribution of $M^{1}$: $\forall m \in \llbracket 1, G_{n} \rrbracket, $
%\[\mathbb{P}_{M}(M = m) := \frac{\exp\left(-3 \cdot \eta \cdot \frac{m}{2} \right) \cdot \prod\limits_{j = 1}^{m} \left(1 + n \cdot \eta \cdot \lambda_{j}^{2}\right)^{2}}{\sum\limits_{k =1}^{G_{n}} \exp\left(-3 \cdot \eta \cdot \frac{k}{2} \right) \cdot \prod\limits_{j = 1}^{k} \left(1 + n \cdot \eta \cdot \lambda_{j}^{2}\right)^{2}}.\]
\[\mathbb{P}_{M^{1}}(M = m) \propto \exp\left(-3 \cdot \eta \cdot \frac{m}{2} \right) \cdot \prod\limits_{j = 1}^{m} \left(1 + n \cdot \eta \cdot \lambda_{j}^{2}\right)^{2}.\]
%\begin{columns} % Subdivide the first main column
%\begin{column}{.5\textwidth} % The first subdivided column within the first main column
Which gives the family of posterior distributions:
\textcolor{red!90!black}{\[\mathbb{P}_{M^{\eta} \vert Y^{\eta}}^{n}(m) \propto \exp\!\!\left[- \frac{\eta}{2} \left( 3 m - \sum\limits_{j = 1}^{m} \frac{\eta\left(Y_{j} \cdot n \cdot \lambda_{j}^{2}\right)^{2}}{1 + \eta \cdot n \cdot \lambda_{j}^{2}} \right)\right].\]}
\end{frame}

\section{Self-informative carrier/limit}
\begin{frame}{Self-informative carrier/limit}
Consider the limit of the family of posteriors as $\eta$ tends to infinite:
\[\textcolor{red!90!black}{\lim_{\eta \rightarrow \infty} \mathbb{P}_{\boldsymbol{\theta}^{\eta} \vert M^{\eta} = m, Y^{\eta}}^{n} = \delta_{\widetilde{\theta}^{m}}},\]
where $\widetilde{\theta}^{m}$ is the projection estimator on the first $m$ dimensions.

The distribution of $M$ tends to a point mass:
\[\textcolor{red!90!black}{\lim_{\eta \rightarrow \infty} \mathbb{P}_{M^{\eta}\vert Y^{\eta}}^{n} = \delta_{\widehat{m}}},\]
where $\widehat{m}$ is the choice given by the frequentist model selection presented earlier.

\medskip

The \textcolor{red}{self-informative limit} is equal to the model selection estimator, $\widehat{\theta}$, presented above.
\end{frame}

\begin{frame}{Self-informative carrier/limit}{Graphic representation}
\begin{figure}
\centering
 \includegraphics[width=.8\linewidth]{iteration.pdf}
\caption{Survival function of $M^{\eta}\vert Y^{\eta}$ for different values of $\eta$}\label{iteration}
\end{figure}
\end{frame}

\section{Formulation of optimality}

\subsection{Frequentist case}
\begin{frame}{Formulation of optimality}{Frequentist case}
\begin{columns}
\begin{column}[T]{.4\textwidth}%
\hspace*{8ex}\includegraphics<1-2>[scale=.8]{inv-gssm-minimax.1}%
\includegraphics<3>[scale=.8]{inv-gssm-minimax.2}% lower
\includegraphics<4>[scale=.8]{inv-gssm-minimax.3}%
\includegraphics<5>[scale=.8]{inv-gssm-minimax.4}%
\includegraphics<6>[scale=.8]{inv-gssm-minimax.5}% upper + lower
\includegraphics<7>[scale=.8]{inv-gssm-minimax.6}%
\includegraphics<8->[scale=.8]{inv-gssm-minimax.7}%
\begin{textblock}{6}(2,14.5) \only<1->{Parameter of interest}\end{textblock} 
\end{column}\begin{column}[T]{.7\textwidth}%
\begin{itemize}
\item<2->
For each frequentist estimator, consider the {\textcolor{red}{maximal risk}} 
over a  class $\Theta^{\circ}$  of parameters
  \begin{equation*}
\sup\limits_{\theta^{\circ} \in \Theta^{\circ}} \mathbb{E}_{\theta^{\circ}}^{n} \left[d\left(\widetilde{\theta}, \theta^{\circ}\right)^{2}\right]
\end{equation*}
 \item<3->  Goal: \textcolor{red!90!black}{derive an upper bound} for this risk
\begin{equation*}
\inf\limits_{\widetilde{\theta}}\sup\limits_{\theta^{\circ} \in \Theta^{\circ}} \mathbb{E}_{\theta^{\circ}}^{n}\left[d\left(\widetilde{\theta}, \theta^{\circ}\right)^{2}\right] \geq \mathcal{R}_{n}^{\star}\left(\Theta^{\circ}\right)
\end{equation*}
\item<8-> and finding $\widehat{\theta}$ such that,
  \begin{equation*}
\sup\limits_{\theta^{\circ}\in \Theta^{\circ}} \mathbb{E}_{\theta^{\circ}}^{n}\left[d\left(\widehat{\theta}, \theta^{\circ}\right)^{2}\right] \leq K^{\star} \cdot \mathcal{R}_{n}^{\star}\left(\Theta^{\circ}\right)
\end{equation*}
 it is then \textcolor{red!90!black}{minimax rate optimal} and \textcolor{red!90!black}{adaptive} if $\widehat{\theta}$ does not depend on $\Theta^{\circ}$.
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\subsection{Pragmatic Bayesian case}
\begin{frame}{Formulation of optimality}{Pragmatic Bayesian paradigm}
How to transfer this in a Bayesian point of view ?

\bigskip

Taking a "pragmatic Bayesian" point of view :
\begin{itemize}
\item $\theta^{\circ}$ the \textcolor{red!90!black}{true parameter}
\item Is $\mathbb{P}_{\boldsymbol{\theta}\vert Y}^{n}$ \textcolor{red!90!black}{shrinking} around $\theta^{\circ}$ as $n$ tends to $\infty$?
\item How fast ?

\end{itemize}

\end{frame}


\begin{frame}{Formulation of optimality}{Pragmatic Bayesian formulation of optimality}
\begin{columns}
\begin{column}[T]{.4\textwidth}%
\hspace*{4ex}\includegraphics<1>[scale=.8]{reg.31}%
\includegraphics<2>[scale=.8]{reg.32}%
\includegraphics<3>[scale=.8]{reg.33}%
\includegraphics<4>[scale=.8]{reg.34}%
\includegraphics<5>[scale=.8]{reg.35}%
\includegraphics<6>[scale=.8]{reg.36}%
\includegraphics<7>[scale=.8]{reg.37}%
\includegraphics<8->[scale=.8]{reg.38}%
\end{column}\begin{column}[T]{.7\textwidth}%

\begin{itemize}

\item<1-> \textcolor{red!90!black}{Concentration rate} $(\phi_{n})_{n \in \mathbb{N}}$
\[\exists c \in \mathbb{R}_{+}, \quad \lim_{n \rightarrow \infty} \mathbb{E}_{\theta^{\circ}}^{n}\left[ \mathbb{P}_{\boldsymbol{\theta}\vert Y}^{n} \left(d\left(\boldsymbol{\theta}, \theta^{\circ} \right)^{2} \geq c \, \phi_{n} \right) \right] = 0 \]

\item<5-> \textcolor{red!90!black}{Exact concentration rate} $(\phi_{n})_{n \in \mathbb{N}}$ if in addition

\[ \lim_{n \rightarrow \infty} \mathbb{E}_{\theta^{\circ}}^{n}\left[ \mathbb{P}_{\boldsymbol{\theta}\vert Y}^{n} \left(d\left(\boldsymbol{\theta}, \theta^{\circ} \right)^{2} \leq c^{-1} \, \phi_{n} \right) \right] = 0 \]

\end{itemize}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Formulation of optimality}{Pragmatic Bayesian formulation of minimax optimality}
\begin{itemize}
\item Uniform concentration rate $(\phi_{n})_{n \in \mathbb{N}}$ over a subset on $\Theta^{\circ} \subset \Theta$
\[  \lim_{n \rightarrow \infty} \sup_{\theta^{\circ} \in \Theta^{\circ}} \mathbb{E}_{\theta^{\circ}}^{n}\left[ \mathbb{P}_{\boldsymbol{\theta}\vert Y}^{n} \left(d\left( \boldsymbol{\theta}, \theta^{\circ} \right)^{2} \geq c \, \phi_{n} \right) \right] = 0\]

\item Exact uniform concentration rate $(\phi_{n})_{n \in \mathbb{N}}$ if in addition
\[ \lim_{n \rightarrow \infty} \sup_{\theta^{\circ} \in \Theta^{\circ}} \mathbb{E}_{\theta^{\circ}}^{n}\left[ \mathbb{P}_{\boldsymbol{\theta}\vert Y}^{n} \left(d\left( \boldsymbol{\theta}, \theta^{\circ} \right)^{2} \leq c^{-1} \, \phi_{n} \right) \right] = 0 \]

\end{itemize}
\end{frame}

\section{Existing optimality results}
\subsection{Definitions}
\begin{frame}{Existing optimality results}{Definitions}
Define the following quantities:
\[\textgoth{b}_{m} := \sum\limits_{j = m + 1}^{\infty} \left(\theta^{\circ}\right)^{2}, \quad \Lambda_{j} := \lambda_{j}^{-2}, \quad m \cdot \overline{\Lambda}_{m} := \sum\limits_{j = 1}^{m} \Lambda_{j},\]
\begin{alignat*}{3}
&m_{n}^{\circ} &&:= \argmin\limits_{m \in \llbracket 1, G_{n} \rrbracket} \left[\textgoth{b}_{m} \vee n^{-1} m \overline{\Lambda}_{m}\right], && \quad \Phi_{n}^{\circ} := \left[\textgoth{b}_{m_{n}^{\circ}}\vee n^{-1} m_{n}^{\circ} \overline{\Lambda}_{m_{n}^{\circ}}\right],\\
&m_{n}^{\star} &&:= \argmin\limits_{m \in \llbracket 1, G_{n} \rrbracket} \left[\textgoth{a}_{m} \vee n^{-1} m \overline{\Lambda}_{m}\right], && \quad \Phi_{n}^{\star} := \left[\textgoth{a}_{m_{n}^{\star}} \vee n^{-1} m_{n}^{\star} \overline{\Lambda}_{m_{n}^{\star}}\right].
\end{alignat*}

It is important to note that:
\begin{itemize}
\item $\Phi_{n}^{\star}$ is the minimax optimal rate over $\Theta^{\circ}$,
\item $\Phi_{n}^{\circ}$ is the oracle optimal rate over the projection estimators.
\end{itemize}

\end{frame}

\subsection{Assumptions}
\begin{frame}{Existing optimality results}{Assumptions}
Define the following assumptions:
\begin{alignat*}{2}
&\left(\mathbb{H}_{\lambda}\right) &&: \exists a \in \mathbb{R}_{+}, c \geq 1 : \quad \forall j \in \mathbb{N}, \quad \left(\frac{1}{c} j^{-a} \leq \lambda_{j} \leq c j^{-a}\right)\\
&\left(\mathbb{H}_{1}\right) &&: 0 <\inf\limits_{n \in \mathbb{N}} \left\{\frac{\left[\textgoth{b}_{m_{n}^{\circ}}\wedge n^{-1} m_{n}^{\circ} \overline{\Lambda}_{m_{n}^{\circ}}\right]}{\left[\textgoth{b}_{m_{n}^{\circ}}\vee n^{-1} m_{n}^{\circ} \overline{\Lambda}_{m_{n}^{\circ}}\right]}\right\} \leq 1\\
&\left(\mathbb{H}_{2}\right) &&: 0 <\inf\limits_{n \in \mathbb{N}} \left\{\frac{\left[\textgoth{a}_{m_{n}^{\star}}\wedge n^{-1} m_{n}^{\star} \overline{\Lambda}_{m_{n}^{\star}}\right]}{\left[\textgoth{a}_{m_{n}^{\star}}\vee n^{-1} m_{n}^{\star} \overline{\Lambda}_{m_{n}^{\star}}\right]}\right\} \leq 1\\
\end{alignat*}
Note that under $\left(\mathbb{H}_{\lambda}\right)$, there exist a constant $L$ such that,
\[\forall m \in \mathbb{N}, \quad \Lambda_{m} \leq L \overline{\Lambda}_{m}.\]

\end{frame}

\begin{frame}{Existing optimality results}{Assumptions}
\textcolor{red}{INSERT EXAMPLES THAT WORK OR NOT FOR $\mathbb{H}_{1}$ AND $\mathbb{H}_{2}$ HERE}
\end{frame}

\subsection{Optimality of the Bayes estimate}
\begin{frame}{Existing optimality results}{Optimality of the Bayes estimate}
In \citet{JJASRS}, it is shown that the estimator $\widehat{\theta}^{\left(1\right)}$ \textcolor{red!90!black}{converges with,}
	\begin{itemize}
		\item \textcolor{red!90!black}{oracle optimal rate} for the quadratic risk which means, $\forall \theta^{\circ} \in \Theta^{\circ}, \exists C^{\circ} \in \left[ 1, \infty \right[ : \forall n \in \mathbb{N}:$
		\begin{alignat*}{2}
&\inf\limits_{m \in \mathbb{N}} \, && \mathbb{E}_{\theta^{\circ}}^{n}\left[\left\Vert \widetilde{\theta}^{m} - \theta^{\circ} \right\Vert^{2}\right] \geq \Phi_{n}^{\circ},\\
& && \mathbb{E}_{\theta^{\circ}}^{n}\left[\left\Vert \widehat{\theta}^{\left(1\right)} - \theta^{\circ} \right\Vert^{2}\right] \leq C^{\circ} \Phi_{n}^{\circ};
		\end{alignat*}
		\item \textcolor{red!90!black}{minimax optimal rate} for the maximal risk over $\Theta^{\circ}$, that is to say, $\exists C^{\star} \in \left[ 1, \infty \right[ : \forall n \in \mathbb{N}:$
\begin{alignat*}{2}
& \inf\limits_{\widetilde{\theta}} &&\sup\limits_{\theta^{\circ} \in \Theta^{\circ}} \mathbb{E}_{\theta^{\circ}}^{n}\left[\left\Vert \widetilde{\theta} - \theta^{\circ} \right\Vert^{2}\right] \geq \Phi_{n}^{\star},\\
& && \sup\limits_{\theta^{\circ} \in \Theta^{\circ}} \mathbb{E}_{\theta^{\circ}}^{n}\left[\left\Vert \widehat{\theta}^{\left(1\right)} - \theta^{\circ} \right\Vert^{2}\right] \leq C^{\star} \Phi_{n}^{\star},
\end{alignat*}
where $\inf\limits_{\widetilde{\theta}}$ is taken over all possible estimators of $\theta^{\circ}$.
	\end{itemize}

\end{frame}

\subsection{Optimality of the posterior distribution}
\begin{frame}{Existing optimality results}{Optimality of the posterior distribution}
It is also shown that the posterior distribution \textcolor{red!90!black}{concentrates with,}
	\begin{itemize}
		\item \textcolor{red!90!black}{oracle optimal rate} for the quadratic loss which means, $\forall \theta^{\circ} \in \Theta^{\circ}, \exists K^{\circ} \in \left[ 1, \infty \right[ :$
\[\lim\limits_{n \rightarrow \infty} \mathbb{E}_{\theta^{\circ}}^{n}\left[\mathbb{P}_{\boldsymbol{\theta}^{1}\vert Y^{1}}^{n}\left(\left\Vert \boldsymbol{\theta} - \theta^{\circ} \right\Vert^{2} \leq K^{\circ} \Phi_{n}^{\circ}\right)\right] = 1;\]
		\item \textcolor{red!90!black}{minimax optimal rate} $\Theta^{\circ}$, that is to say, for any unbounded sequence $K_{n} \in \mathbb{R}^{\mathbb{N}} :$
\[\lim\limits_{n \rightarrow \infty} \sup\limits_{\theta^{\circ} \in \Theta^{\circ}}  \mathbb{E}_{\theta^{\circ}}^{n}\left[\mathbb{P}_{\boldsymbol{\theta}^{1}\vert Y^{1}}^{n}\left(\left\Vert \boldsymbol{\theta} - \theta^{\circ} \right\Vert^{2} \leq K_{n} \Phi_{n}^{\star}\right)\right] = 1.\]
	\end{itemize}
\end{frame}

\section{Optimality of the iterated version}
\subsection{Optimality of the hyper-parameter}
\begin{frame}{Optimality of the iterated version}{Optimality of the hyper-parameter - oracle optimal choice}
For any $\eta$ in $\overline{\mathbb{N}},$ we have the following results:

\medskip

Under assumptions $\left(\mathbb{H}_{1}\right)$ and $\left(\mathbb{H}_{\lambda}\right)$, define
\begin{alignat*}{2}
&G_{n}^{-} &&:= \min\left\{m \in \llbracket 1, m_{n}^{\circ} \rrbracket : \textgoth{b}_{m} \leq 9 L \Phi_{n}^{\circ}\right\},\\
&G_{n}^{+} &&:= \max \left\{m \in \llbracket m_{n}^{\circ}, G_{n} \rrbracket : \left( m - m_{n}^{\circ} \right) n^{-1} \leq 3 \Lambda_{m_{n}^{\circ}}^{-1} \Phi_{n}^{\circ}\right\},
\end{alignat*}
and we then have the following concentration for $M$,
\begin{alignat*}{4}
&\mathbb{P}_{\theta^{\circ}}&&\left[M^{\eta} > G_{n}^{+}\right] &&\leq&& \exp\left[- \frac{5 m_{n}^{\circ}}{9 L} + \log \left(G_{n}\right)\right],\\
&\mathbb{P}_{\theta^{\circ}}&&\left[M^{\eta} < G_{n}^{-}\right] &&\leq&& \exp\left[- \frac{7 m_{n}^{\circ}}{9} + \log \left(G_{n}\right)\right],
\end{alignat*}
\textcolor{red!90!black}{this means that $M^{\eta}$ tends to select an oracle optimal threshold};


\end{frame}
\begin{frame}{Optimality of the iterated version}{Optimality of the hyper-parameter- minimax optimal choice}
For any $\eta$ in $\overline{\mathbb{N}},$ we have the following results:

under $\left(\mathbb{H}_{2}\right)$ and $\left(\mathbb{H}_{\lambda}\right)$, we define
\begin{alignat*}{2}
&G_{n}^{\star-} &&:= \min\left\{m \in \llbracket 1, m_{n}^{\star} \rrbracket : \quad \textgoth{b}_{m} \leq 9 \left(1 \vee L^{\circ}\right) L \Phi_{n}^{\star}\right\},\\
&G_{n}^{\star+} &&:= \max \left\{m \in \llbracket m_{n}^{\star}, G_{n} \rrbracket : \left( m - m_{n}^{\star} \right) n^{-1} \leq 3 \Lambda_{m_{n}^{\star}}^{-1} \left(1 \vee L^{\circ}\right) \Phi_{n}^{\star}\right\},
\end{alignat*}
and the following concentration stands,
\begin{alignat*}{4}
&\mathbb{P}_{\theta^{\circ}}&&\left[M^{\eta} > G_{n}^{\star+}\right] &&\leq &&\exp\left[- \frac{5 \left(1 \vee L^{\circ}\right) m_{n}^{\star}}{9 L} + \log \left(G_{n}\right)\right],\\
&\mathbb{P}_{\theta^{\circ}}&&\left[M^{\eta} < G_{n}^{\star-}\right] &&\leq &&\exp\left[- \frac{7 \left(1 \vee L^{\circ}\right) m_{n}^{\star}}{9} + \log \left(G_{n}\right)\right],
\end{alignat*}
\textcolor{red!90!black}{which means that $M^{\eta}$ tends to select a minimax optimal threshold}.
\end{frame}


\subsection{Optimality of the iterated Bayes estimate}
\begin{frame}{Optimality of the iterated version}{Optimality of the iterated Bayes estimate}
For any $\eta$ in $\mathbb{N},$ the following results stand:
\begin{itemize}
	\item under $\mathbb{H}_{\lambda}$ and $\mathbb{H}_{1};$ for all $\theta^{\circ}$ in $\Theta^{\circ}$, there exist $C^{\circ}$ in $\left[1, \infty \right[$ such that
	\[\mathbb{E}_{\theta^{\circ}}^{n}\left[\Vert \widehat{\theta}^{\left(\eta\right)} - \theta^{\circ} \Vert^{2}\right] \leq C^{\circ} \Phi_{n}^{\circ};\]
	\item under $\mathbb{H}_{\lambda}$ and $\mathbb{H}_{2};$ there exist $C^{\star}$ in $\left[1, \infty\right[$ such that
	\[\sup\limits_{\theta^{\circ}\in \Theta^{\circ}}\mathbb{E}_{\theta^{\circ}}^{n}\left[\Vert \widehat{\theta}^{\left(\eta\right)} - \theta^{\circ} \Vert^{2}\right] \leq C^{\star} \Phi_{n}^{\star}.\]
\end{itemize}
Hence, $\widehat{\theta}^{\left(\eta\right)}$ has oracle optimal rate of the projection estimators and minimax optimal rate over $\Theta^{\circ}$.

This also stands true for the self informative limit.
\end{frame}

\subsection{Optimality of the iterated posterior}
\begin{frame}{Optimality of the iterated version}{Optimality of the iterated posterior}
First, we underline that $\Phi_{n}^{\circ}$ and $\Phi_{n}^{\star}$ can be considered as upper bounds for the family of sieve priors:
\begin{itemize}
\item $\lim\limits_{n \rightarrow \infty} \inf\limits_{\mathbb{Q}_{\boldsymbol{\theta}}}\, \mathbb{E}_{\theta^{\circ}}^{n}\left[\mathbb{Q}_{\boldsymbol{\theta} \vert Y}^{n}\left(\left\Vert \boldsymbol{\theta} - \theta^{\circ} \right\Vert^{2} \geq \Phi_{n}^{\circ}\right)\right] = 1$,
\item $\lim\limits_{n \rightarrow \infty} \inf\limits_{\mathbb{Q}_{\boldsymbol{\theta}}}\sup\limits_{\theta^{\circ} \in \Theta^{\circ}}\, \mathbb{E}_{\theta^{\circ}}^{n}\left[\mathbb{Q}_{\boldsymbol{\theta} \vert Y}^{n}\left(\left\Vert \boldsymbol{\theta} - \theta^{\circ} \right\Vert^{2} \geq \Phi_{n}^{\star}\right)\right] = 1$,
\end{itemize}
where $\inf_{\mathbb{Q}_{\boldsymbol{\theta}}}$ is taken over all possible sieve priors.
\end{frame}

\begin{frame}{Optimality of the iterated version}{Optimality of the iterated posterior}
For any $\eta$ in $\mathbb{N},$ the following results stand:
\begin{itemize}
	\item under $\mathbb{H}_{\lambda}$ and $\mathbb{H}_{1};$ for all $\theta^{\circ}$ in $\Theta^{\circ}$, there exist $K^{\circ}$ in $\left[1, \infty \right[$ such that
	\[\lim\limits_{n \rightarrow \infty} \mathbb{E}_{\theta^{\circ}}^{n}\left[\mathbb{P}_{\boldsymbol{\theta}^{\eta}, M^{\eta} \vert Y^{\eta}}^{n}\left(\left(K^{\circ}\right)^{-1} \Phi_{n}^{\circ} \leq \Vert \boldsymbol{\theta} - \theta^{\circ} \Vert^{2} \leq K^{\circ} \Phi_{n}^{\circ} \right)\right] = 1;\]
	\item under $\mathbb{H}_{\lambda}$ and $\mathbb{H}_{2};$ for any unbounded sequence $K_{n}$, we have
	\[\lim\limits_{n \rightarrow \infty} \sup\limits_{\theta^{\circ} \in \Theta^{\circ}} \mathbb{E}_{\theta^{\circ}}^{n}\left[\mathbb{P}_{\boldsymbol{\theta}^{\eta}, M^{\eta} \vert Y^{\eta}}^{n}\left(\Vert \boldsymbol{\theta} - \theta^{\circ} \Vert^{2} \leq K_{n} \Phi_{n}^{\star} \right)\right] = 1.\]
\end{itemize}
Establishing oracle and minimax optimal concentration of the posterior distributions of the family.

This also stands true for the self informative Bayes carrier, which corresponds to convergence in probability, indeed:
\[\lim\limits_{\eta \rightarrow \infty}\mathbb{E}_{\theta^{\circ}}^{n}\left[\mathbb{P}_{\boldsymbol{\theta}^{\eta}, M^{\eta} \vert Y^{\eta}}^{n}\left(\Vert \boldsymbol{\theta} - \theta^{\circ} \Vert^{2} \leq K_{n} \Phi_{n} \right)\right] = \mathbb{P}_{\theta^{\circ}}^{n}\left[\Vert \widehat{\theta} - \theta^{\circ} \Vert^{2} \leq K_{n} \Phi_{n}\right].\]
\end{frame}

\section{Simulations}
\begin{frame}{Simulations}
\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{EQM1.pdf}
  \caption{$\theta^{\circ}$ polynomial and $\lambda$ polynomial}
  \label{fig3:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{EQM2.pdf}
  \caption{$\theta^{\circ}$ polynomial and $\lambda$ exponential}
  \label{fig3:sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{EQM3}
  \caption{$\theta^{\circ}$ exponential and $\lambda$ polynomial}
  \label{fig3:sub3}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{EQM4}
  \caption{$\theta^{\circ}$ exponential and $\lambda$ exponential}
  \label{fig3:sub4}
\end{subfigure}
\caption{Estimated mean squared error of the estimator given by the posterior mean for different classes of $\theta^{\circ}$ and $\lambda$.}
\label{EQM}
\end{figure}
\end{frame}
\begin{frame}{Bib}
\bibliography{biblio.bib}{}
\end{frame}
\bibliographystyle{plainnat}
\end{document}