\section{Iterated Gaussian sieve prior}\label{2.1}

We consider in this part a statistical model with a functional parameter space as described in \nref{1.1.1}.
We adopt a sieve prior as described in \nref{1.3.2} and first give interest to the asymptotic presented in \nref{1.3.5}.

\medskip

We first remind the following notations.
The parameter space $\Theta$ is a function space $\Theta = \{ \theta : \mathds{F} \rightarrow \mathds{I} \}$; with $\mathds{F}$ a subset of $\R$ and $\mathds{I}$ a subset of $\C$.

To derive the self informative Bayes carrier we formulate the following hypothesis.

\begin{as}{\textsc{Countability assumption}\\}\label{as2.1.1}
We assume that the set $\mathds{F}$ is countable.
\end{as}

We equip $\Theta$ with the usual $\mathds{L}^{2}$ norm that is, $\Vert \theta \Vert^{2} = \sum\limits_{j \in \mathds{J}} \vert \theta_{j} \vert^{2}$ and consider the Borel sigma algebra $\mathcal{B}$ of the topology generated by this $\mathds{L}^{2}$ norm.

On the other hand our observation $Y$ take values in the space $(\mathds{Y}, \mathcal{Y})$ with distribution in the family $(\P_{Y \vert \boldsymbol{\theta}})_{\boldsymbol{\theta} \in \Theta}$.


We assume the existence of a function $l: (\Theta, \mathcal{B}) \times (\mathds{Y}, \mathcal{Y}) \rightarrow \R$ such that the likelihood with respect to some reference measures $\P^{\circ}$ is given by:

\[L(\boldsymbol{\theta}, y) \propto \exp\left[-l(\boldsymbol{\theta}, y)\right].\]

Then, the family of Gaussian sieve priors is indexed by a threshold parameter $m$ in the set of subsets of $\mathds{J}$, denoted $\mathcal{P}(\mathds{J})$, and we denote by $\P_{\boldsymbol{\theta}^{m}}$ the element of this family with index $m$; moreover, we denote $\boldsymbol{\theta}^{m}$ a random variable following this distribution. 
There exists a reference measure $\Q^{\circ}$ such that the sieve prior with threshold parameter $m$ admits a density of the shape

\[\frac{d\P_{\boldsymbol{\theta}^{m}}}{d\Q^{\circ}}(\boldsymbol{\theta}) \propto  \exp\left[-\frac{1}{2}\sum\limits_{j \in m} \vert \boldsymbol{\theta}_{j} \vert^{2}\right] \cdot \prod\limits_{j \notin m} \delta_{0}(\boldsymbol{\theta}_{j}).\]

If we denote by $\Theta_{m}$ the set $\{\theta \in \Theta : \forall j \notin m, \theta_{j} = 0\}$, Bayes' theorem gives the following shape for the iterated posterior distribution:

\begin{alignat*}{3}
& \frac{d\P_{\boldsymbol{\theta}^{m}\vert Y}^{\eta}}{d\Q^{\circ}}(\boldsymbol{\theta}, y)&& = && \frac{\exp\left[-\left(\frac{1}{2}\sum\limits_{j \in m} \vert \boldsymbol{\theta}_{j} \vert^{2} + \eta l(\boldsymbol{\theta}, y)\right)\right] \cdot \prod\limits_{j \notin m} \delta_{0}(\boldsymbol{\theta}_{j})}{\int_{\Theta_{m}} \exp\left[-\left(\frac{1}{2}\sum\limits_{j \in m} \vert \mu_{j} \vert^{2} + \eta l(\mu, y)\right)\right] d\mu}\\
& && = && \frac{\prod\limits_{j \notin m} \delta_{0}(\boldsymbol{\theta}_{j})}{\int_{\Theta_{m}} \exp\left[-\frac{1}{2}\sum\limits_{j \in m} \left(\vert \mu_{j} \vert^{2} - \vert \boldsymbol{\theta}_{j} \vert^{2}\right)\right]\exp\left[-\eta\left(l(\mu, y) - l(\boldsymbol{\theta}, y)\right)\right] d\mu}.
\end{alignat*}

The following assumption is also needed to obtain the self informative Bayes carrier.

\begin{as}{\textsc{Continuous likelihood asumption}\\}\label{as2.1.2}
Assume that for any $m$ in $\mathcal{P}(\mathds{J})$ and $y$, $\Theta_{m} \rightarrow \R_{+}, \theta \mapsto l(\theta, y)$ is continuous.
\end{as}

The use of a threshold parameter brings us back to the study of a parametric model and the results from \textcolor{red}{ref Bunke} can be used to derive the self informative Bayes carrier.

\begin{thm}{\textsc{Self informative Bayes carrier for a sieve prior}\\}\label{thm2.1.1}
Under \nref{as2.1.1} and \nref{as2.1.2} the support of the Bayesian carrier is contained in the set of minimisers of $\theta \mapsto l(\theta, y)$.
\end{thm}

\begin{pro}{\textsc{Proof of \nref{thm2.1.1}}\\}\label{pro2.1.1}
Let's remind that the definition of continuity gives us:
\[\forall \theta \in \Theta_{m}, \forall \epsilon \in \R_{+}^{\star}, \exists \delta \in \R_{+}^{\star} : \forall \mu \in \Theta_{m}, \Vert \mu - \theta \Vert < \delta \Rightarrow \vert l(\mu, y) - l(\theta, y) \vert < \epsilon.\]

\medskip

Then, for any $B$ in $\mathcal{B}$ such that $\inf\limits_{\theta \in B} l(\theta, y) > \inf\limits_{\mu \in \Theta_{m}} l(\mu, y)$, there exist $\delta$ in $\R_{+}^{\star}$ and a ball $\mathcal{E}$ of $\Theta_{m}$ of radius $\delta$ such that, $\sup\limits_{\mu \in \mathcal{E}} l(\mu, y) < \inf\limits_{\theta \in B}l(\theta, y)$ and hence $\sup\limits_{\mu \in \mathcal{E}}l(\mu, y) - \inf\limits_{\theta \in B}l(\theta, y) < 0$.

Hence we can write
\begin{alignat*}{3}
& \P_{\boldsymbol{\theta}^{m}\vert Y}^{\eta}(B) && = && \int_{B} \frac{\prod\limits_{\vert j \vert > m} \delta_{0}(\boldsymbol{\theta}_{j})}{\int_{\Theta_{m}} \exp\left[-\frac{1}{2}\sum\limits_{\vert j \vert \leq m} \left(\vert \mu_{j} \vert^{2} - \vert \boldsymbol{\theta}_{j} \vert^{2}\right)\right]\exp\left[-\eta\left(l(\mu, y) - l(\boldsymbol{\theta}, y)\right)\right] d\mu} d \theta\\
& && \leq && \int_{B} \frac{\prod\limits_{\vert j \vert > m} \delta_{0}(\theta_{j})}{\exp\left[-\eta\left(\sup\limits_{\mu \in \mathcal{E}} l(\mu, y) - \inf\limits_{\theta \in B}l(\theta, y)\right)\right] \int_{\mathcal{E}} \exp\left[-\frac{1}{2}\sum\limits_{\vert j \vert \leq m} \left(\vert \mu_{j} \vert^{2} - \vert \boldsymbol{\theta}_{j} \vert^{2}\right)\right]d\mu} d \theta\\
& && \leq && \frac{1}{\exp\left[-\eta\left(\sup\limits_{\mu \in \mathcal{E}} l(\mu, y) - \inf\limits_{\theta \in B}l(\theta, y)\right)\right]}\int_{B} \frac{\prod\limits_{\vert j \vert > m} \delta_{0}(\theta_{j}) \exp\left[-\frac{1}{2}\sum\limits_{\vert j \vert \leq m} \vert \boldsymbol{\theta}_{j} \vert^{2}\right]}{ \int_{\mathcal{E}} \exp\left[-\frac{1}{2}\sum\limits_{\vert j \vert \leq m} \vert \mu_{j} \vert^{2}\right]d\mu} d \theta\\
& && \rightarrow && 0.
\end{alignat*}
\qedsymbol
\end{pro}

We have hence showed that under the iteration asymptotic, the posterior distribution contracts itself on maximisers of the likelihood, constrained by $\theta_{j} = 0$ for any $\vert j \vert > m$.

\textcolor{red}{Add remark with several maximisers}

There is hence a clear link between this type of prior distribution and projection estimators.
We will see that, while considering the noise asymptotic, the choice of the threshold is determinant for the quality of the estimation.
The choice of the threshold for the projection estimators and for sieve priors should be led in a similar fashion, that is, balancing the bias (small value of the threshold) and the variance (high value of the threshold).
As stated previously, the ideal choice of this parameter is however dependent on the parameter of interest and hence not available.
It is hence important to inquire adaptive methods for the selection of this parameter.
Some methods for the frequentist estimation were outlined in the introduction such as the penalised contrast model selection.
In the next section, we introduce the hierarchical sieve prior which consists in modelling the threshold parameter as a random variable.
We will show that by selecting the prior distribution for this hyper-parameter properly, the iteration asymptotic gives a Bayesian interpretation to the penalised contrast model selection.
